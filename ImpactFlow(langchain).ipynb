{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMk6pqLHbDCkIIzBgLtlnl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Junghyun1217/ImpactFlow-Streamlined-automation-for-social-value-evaluation/blob/main/ImpactFlow(langchain).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4OY20AiQPXE"
      },
      "outputs": [],
      "source": [
        "#Pararrel\n",
        "# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "import getpass\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import glob\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Optional\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# ë°ì´í„° ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.patheffects as patheffects\n",
        "from matplotlib.patches import Rectangle\n",
        "import seaborn as sns\n",
        "\n",
        "# LangChain ë° AI ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "from dotenv import load_dotenv\n",
        "from llama_parse import LlamaParse\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from langchain.tools import tool\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph_supervisor import create_supervisor\n",
        "\n",
        "# import gethwp\n",
        "\n",
        "load_dotenv(\".env\", override=True)\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "_set_env(\"LLAMA_CLOUD_API_KEY\")\n",
        "\n",
        "# print(os.environ.get(\"OPENAI_API_KEY\")[:10])\n",
        "# print(os.environ.get(\"LLAMA_CLOUD_API_KEY\")[:10])\n",
        "\n",
        "\n",
        "DOCUMENT_PATH = \"docs\"\n",
        "## check\n",
        "SDGS_SUBGOALS_PATH = 'data/SDGs_json/SDGs_subgoals.json'\n",
        "GRI_DETAIL_PATH = 'data/gri/gri_detail.json' # <-- ì´ ì¤„ì„ ì¶”ê°€í•˜ì„¸ìš”\n",
        "\n",
        "MODEL_NAME = \"gpt-4o-mini\"  # ìœ íš¨í•œ ëª¨ë¸ëª…ìœ¼ë¡œ ìˆ˜ì •\n",
        "TOPN_SELECT = 3\n",
        "MAX_WORKERS = 8 # ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ê°œìˆ˜ (í•„ìš”ì— ë§ê²Œ ì¡°ì ˆ)\n",
        "\n",
        "## check\n",
        "# with open('reference/SDGs_subgoals.json', 'r') as f:\n",
        "with open('data/SDGs_json/SDGs_subgoals.json', 'r', encoding='utf-8') as f:\n",
        "    SDGS_SUBGOALS = json.load(f)\n",
        "\n",
        "# llm = ChatOpenAI(model=\"gpt-4.1\", temperature = 0.1)\n",
        "llm = ChatOpenAI(model=\"gpt-5-mini\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# ì„¹ì…˜ A: SDGs ë¶„ì„ ê´€ë ¨ í•¨ìˆ˜ë“¤\n",
        "# =================================================================\n",
        "#=====================================================#\n",
        "#================ Step1: ê¸°ì—… ì •ë³´ ì…ë ¥ ==================#\n",
        "#=====================================================#\n",
        "def process_file(file_path, ext, parser):\n",
        "    try:\n",
        "        if ext == \"hwp\":\n",
        "            docs = gethwp.read_hwp(file_path)\n",
        "        elif ext == \"hwpx\":\n",
        "            docs = gethwp.read_hwpx(file_path)\n",
        "        else:\n",
        "            docs = parser.load_data(file_path=file_path)\n",
        "        texts = \"\"\n",
        "        for doc in docs:\n",
        "            texts += doc.text\n",
        "        return texts\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] {file_path} ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def read_document(file_dir):\n",
        "    print('='*100)\n",
        "    print(\"Step1: ê¸°ì—… ì •ë³´ ì…ë ¥\")\n",
        "\n",
        "    parser = LlamaParse(\n",
        "        use_vendor_multimodal_model=True,\n",
        "        vendor_multimodal_model_name=\"openai-gpt-4-1-mini\",\n",
        "        vendor_multimodal_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "        result_type=\"text\",\n",
        "        language=\"ko\",\n",
        "    )\n",
        "\n",
        "    target_exts = [\"pdf\", \"pptx\", \"doc\", \"docx\", \"hwp\", \"hwpx\"]\n",
        "    file_tasks = []\n",
        "\n",
        "    for ext in target_exts:\n",
        "        file_paths = glob.glob(os.path.join(file_dir, f\"*.{ext}\"))\n",
        "        for file_path in file_paths:\n",
        "            file_tasks.append((file_path, ext))\n",
        "\n",
        "    company_info = \"\"\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        futures = [executor.submit(process_file, fp, ext, parser) for fp, ext in file_tasks]\n",
        "        for future in futures:\n",
        "            company_info += future.result()\n",
        "\n",
        "    return company_info\n",
        "#=====================================================#\n",
        "#================ Step2: ê¸°ì—… ì •ë³´ ë§ˆìŠ¤í‚¹ ================#\n",
        "#=====================================================#\n",
        "\n",
        "def mask_info(company_info):\n",
        "    print('='*100)\n",
        "    print(\"Step2: ê¸°ì—… ì •ë³´ ë§ˆìŠ¤í‚¹ (í˜„ì¬ ê·¸ëŒ€ë¡œ return)\")\n",
        "    return company_info\n",
        "\n",
        "#=====================================================#\n",
        "#================ Step3: ê¸°ì—… ì •ë³´ ìš”ì•½ ================#\n",
        "#=====================================================#\n",
        "\n",
        "def summary_sv(text: str) -> str:\n",
        "    print('='*100)\n",
        "    print(\"Step3: ê¸°ì—… ì •ë³´ ìš”ì•½\")\n",
        "    \"\"\"\n",
        "    ì£¼ì–´ì§„ ë³´ê³ ì„œ ë¬¸ë‹¨ì„ ë¶„ì„í•˜ì—¬ ê¸°ì—… ë˜ëŠ” ë‹¨ì²´ê°€ ì°½ì¶œí•˜ëŠ” ì‚¬íšŒì  ê°€ì¹˜ë¥¼ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.\n",
        "    ìš”ì•½ ì‹œ ë‹¤ìŒ ìš”ì†Œë¥¼ í¬í•¨í•©ë‹ˆë‹¤:\n",
        "      - í•´ê²°í•˜ë ¤ëŠ” ì‚¬íšŒë¬¸ì œ\n",
        "      - ì œê³µí•˜ëŠ” ì†”ë£¨ì…˜\n",
        "      - ê¸°ëŒ€ë˜ëŠ” ì‚¬íšŒì  íš¨ê³¼\n",
        "    ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ë‚˜ ì¥í™©í•œ ì„¤ëª…ì€ ì œì™¸í•˜ê³ , í•µì‹¬ë§Œ ëª…í™•í•˜ê²Œ ê¸°ìˆ í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    ì•„ë˜ì˜ ë¬¸ë‹¨ì„ ì½ê³ , ê¸°ì—… ë˜ëŠ” ë‹¨ì²´ê°€ ì°½ì¶œí•˜ëŠ” ì‚¬íšŒì  ê°€ì¹˜ë¥¼ í•œ ë¬¸ì¥ ë˜ëŠ” ë‘ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì‹œì˜¤.\n",
        "    ë°˜ë“œì‹œ ë‹¤ìŒ ìš”ì†Œë¥¼ í¬í•¨í•˜ì‹­ì‹œì˜¤:\n",
        "    1) í•´ê²°í•˜ë ¤ëŠ” ì‚¬íšŒë¬¸ì œ\n",
        "    2) ì œê³µí•˜ëŠ” ì†”ë£¨ì…˜ ë˜ëŠ” í™œë™\n",
        "    3) ê¸°ëŒ€ë˜ëŠ” ì‚¬íšŒì Â·í™˜ê²½ì  íš¨ê³¼\n",
        "    ì£¼ê´€ì  í‰ê°€ë‚˜ ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ëŠ” ë°°ì œí•˜ê³ , ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ.\n",
        "\n",
        "ì˜ˆì‹œ: \"ê·¸ë¦°ì¸í”„ë¼ìŠ¤íŠ¸ëŸ­ì²˜ëŠ” ë„ì‹¬ ë‚´ ìœ íœ´ë¶€ì§€ì— ë„ì‹œë†ì—…ê³¼ ì‘ì€ ìˆ² ì¡°ì„± í”„ë¡œì íŠ¸ë¥¼ ì‹œí–‰í•´ ë¯¸ì„¸ë¨¼ì§€ ì €ê°ê³¼ ì£¼ë¯¼ íœ´ì‹ê³µê°„ í™•ëŒ€ë¥¼ ì¶”ì§„í•©ë‹ˆë‹¤. ì§€ì—­ ì£¼ë¯¼ê³¼ í•¨ê»˜ ê´€ë¦¬ê³„íšì„ ìˆ˜ë¦½í•´ ìƒë¬¼ë‹¤ì–‘ì„± íšŒë³µê³¼ ê³µê³µ ê³µê°„ì˜ ì‚¬íšŒì  ê°€ì¹˜ë¥¼ ë†’ì…ë‹ˆë‹¤.\"\n",
        "\n",
        "[ë¬¸ë‹¨]\n",
        "{text}\n",
        "    \"\"\"\n",
        "    response = llm.invoke(prompt,seed=23).content\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "#=====================================================#\n",
        "#============= Step4:SDGs Main Code íŒë³„ ==============#\n",
        "#=====================================================#\n",
        "def match_sdgs_by_llm(text: str) -> dict:\n",
        "    print('='*100)\n",
        "    print(\"Step4:SDGs Main Code íŒë³„\")\n",
        "\n",
        "    \"\"\"\n",
        "    GPTê°€ ë³´ê³ ì„œ ë¬¸ë‹¨ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ SDGs ëª©í‘œë¥¼ 1~2ê°œ ì¶”ë¡ í•©ë‹ˆë‹¤.\n",
        "    ê° ì½”ë“œì— ëŒ€í•´ ê°„ë‹¨í•œ ì´ìœ ì™€ í•¨ê»˜ ì„¤ëª…ì„ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "    ì˜ˆ: {\"2\": \"ê¸°ì•„ ì¢…ì‹ â€” ì•„ë™ ì˜ì–‘ ë¬¸ì œ í•´ì†Œì™€ ê´€ë ¨ ìˆìŒ\"}\n",
        "    \"\"\"\n",
        "\n",
        "    sdgs_definitions = \"\"\"\n",
        "    SDGs ì •ì˜ ëª©ë¡:\n",
        "    - SDGs 1: ëª¨ë“  ê³³ì—ì„œ ëª¨ë“  í˜•íƒœì˜ ë¹ˆê³¤ ì¢…ì‹\n",
        "    - SDGs 2: ê¸°ì•„ ì¢…ì‹, ì•ˆì „í•˜ê³  ì˜ì–‘ì´ ê°œì„ ëœ ì‹ëŸ‰ ë‹¬ì„±, ì§€ì†ê°€ëŠ¥í•œ ë†ì—… ë°œì „\n",
        "    - SDGs 3: ëª¨ë“  ì—°ë ¹ì¸µì„ ìœ„í•œ ê±´ê°•í•œ ì‚¶ ë³´ì¥ê³¼ ì›°ë¹™ ì¦ì§„\n",
        "    - SDGs 4: í¬ìš©ì ì´ê³  ê³µí‰í•œ ì–‘ì§ˆì˜ êµìœ¡ì„ ë³´ì¥í•˜ê³  ëª¨ë‘ë¥¼ ìœ„í•œ í‰ìƒ í•™ìŠµ ê¸°íšŒë¥¼ ì´‰ì§„í•©ë‹ˆë‹¤\n",
        "    - SDGs 5: ì„±í‰ë“± ë‹¬ì„±ê³¼ ëª¨ë“  ì—¬ì„± ë° ì—¬ì•„ì˜ ê¶Œìµì‹ ì¥\n",
        "    - SDGs 6: ëª¨ë‘ë¥¼ ìœ„í•œ ë¬¼ê³¼ ìœ„ìƒì˜ ì´ìš©ê°€ëŠ¥ì„±ê³¼ ì§€ì†ê°€ëŠ¥í•œ ê´€ë¦¬ ë³´ì¥\n",
        "    - SDGs 7: ëª¨ë‘ë¥¼ ìœ„í•œ ì ì •ê°€ê²©ì˜ ì‹ ë¢°í•  ìˆ˜ ìˆê³  ì§€ì†ê°€ëŠ¥í•˜ë©° í˜„ëŒ€ì ì¸ ì—ë„ˆì§€ì— ëŒ€í•œ ì ‘ê·¼ ë³´ì¥\n",
        "    - SDGs 8: ì§€ì†ì ì´ê³  í¬ìš©ì ì´ë©° ì§€ì† ê°€ëŠ¥í•œ ê²½ì œ ì„±ì¥, ì™„ì „í•˜ê³  ì§€ì† ê°€ëŠ¥í•œ ìƒì‚°ì ì¸ ê³ ìš©ê³¼ ëª¨ë‘ë¥¼ ìœ„í•œ ì–‘ì§ˆì˜ ì¼ìë¦¬ ì¦ì§„\n",
        "    - SDGs 9: íšŒë³µë ¥ ìˆëŠ” ì‚¬íšŒê¸°ë°˜ì‹œì„¤ êµ¬ì¶•, í¬ìš©ì ì´ê³  ì§€ì†ê°€ëŠ¥í•œ ì‚°ì—…í™” ì¦ì§„ê³¼ í˜ì‹  ë„ëª¨\n",
        "    - SDGs 10: êµ­ê°€ ë‚´ ë° êµ­ê°€ ê°„ ë¶ˆí‰ë“± ê°ì†Œ\n",
        "    - SDGs 11: í¬ìš©ì ì´ê³  ì•ˆì „í•˜ë©° íšŒë³µë ¥ ìˆê³  ì§€ì†ê°€ëŠ¥í•œ ë„ì‹œì™€ ì£¼ê±°ì§€ ì¡°ì„±\n",
        "    - SDGs 12: ì§€ì†ê°€ëŠ¥í•œ ì†Œë¹„ì™€ ìƒì‚° ì–‘ì‹ì˜ ë³´ì¥\n",
        "    - SDGs 13: ê¸°í›„ë³€í™”ì™€ ê·¸ë¡œ ì¸í•œ ì˜í–¥ì— ë§ì„œê¸° ìœ„í•œ ê¸´ê¸‰ ëŒ€ì‘\n",
        "    - SDGs 14: ì§€ì†ê°€ëŠ¥ë°œì „ì„ ìœ„í•˜ì—¬ ëŒ€ì–‘, ë°”ë‹¤, í•´ì–‘ìì›ì˜ ë³´ì „ê³¼ ì§€ì†ê°€ëŠ¥í•œ ì´ìš©\n",
        "    - SDGs 15: ìœ¡ìƒìƒíƒœê³„ ë³´í˜¸, ë³µì› ë° ì§€ì†ê°€ëŠ¥í•œ ì´ìš© ì¦ì§„, ì§€ì†ê°€ëŠ¥í•œ ì‚°ë¦¼ ê´€ë¦¬, ì‚¬ë§‰í™” ë°©ì§€, í† ì§€í™©íí™” ì¤‘ì§€ì™€ íšŒë³µ, ìƒë¬¼ë‹¤ì–‘ì„± ì†ì‹¤ ì¤‘ë‹¨\n",
        "    - SDGs 16: ì§€ì†ê°€ëŠ¥ë°œì „ì„ ìœ„í•œ í‰í™”ë¡­ê³  í¬ìš©ì ì¸ ì‚¬íšŒ ì¦ì§„, ëª¨ë‘ì—ê²Œ ì •ì˜ ë³´ì¥ê³¼ ëª¨ë“  ìˆ˜ì¤€ì—ì„œ íš¨ê³¼ì ì´ê³  ì±…ì„ì„± ìˆìœ¼ë©° í¬ìš©ì ì¸ ì œë„ êµ¬ì¶•\n",
        "    - SDGs 17: ë‚˜ë¨¸ì§€ ëª©í‘œë“¤ì˜ ì‹¤í–‰ìˆ˜ë‹¨ ê°•í™”ì™€ ì§€ì†ê°€ëŠ¥ë°œì „ì„ ìœ„í•œ ê¸€ë¡œë²Œ íŒŒíŠ¸ë„ˆì‹­ ì¬í™œì„±í™”\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "\n",
        "ë„ˆëŠ” ì‚¬íšŒì  ê°€ì¹˜ í‰ê°€ ë° SDGs ì „ë¬¸ê°€ì•¼.\n",
        "ë‹¤ìŒì€ SDGs ëª©í‘œ ì •ì˜ ëª©ë¡ì´ì•¼: {sdgs_definitions}\n",
        "\n",
        "ì•„ë˜ ë¬¸ë‹¨ì„ ì½ê³ , ì´ì™€ ê°€ì¥ ê´€ë ¨ ìˆëŠ” SDGs ëª©í‘œë¥¼ 1~2ê°œ ì¶”ë¡ í•´ì¤˜.\n",
        "ê° SDGs ì½”ë“œ(ìˆ«ì)ì™€ ê°„ë‹¨í•œ ì´ìœ ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´.\n",
        "\n",
        "í˜•ì‹ ì˜ˆì‹œ:\n",
        "{{\n",
        "  \"2\": \"ê¸°ì•„ ì¢…ì‹ â€” ì•„ë™ ì˜ì–‘ ë¬¸ì œ í•´ì†Œì™€ ê´€ë ¨ ìˆìŒ\",\n",
        "  \"3\": \"ê±´ê°•ê³¼ ì›°ë¹™ â€” ì‹ ì²´ ë° ì •ì‹  ê±´ê°• ê°œì„ ê³¼ ê´€ë ¨\"\n",
        "}}\n",
        "\n",
        "[ë¬¸ë‹¨]\n",
        "{text}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt,seed=23).content\n",
        "    return response\n",
        "\n",
        "#=====================================================#\n",
        "#============ Step5-1 :SDGs Sub Code íŒë³„ =============#\n",
        "#=====================================================#\n",
        "def match_sdgs_sub_by_llm(selected_sdgs_goal: str) -> dict:\n",
        "    print('='*100)\n",
        "    print(\"Step5-1:SDGs Sub Code íŒë³„\")\n",
        "    \"\"\"\n",
        "    ì´ë¯¸ ì„ íƒëœ SDGs ëŒ€ì œëª©(ì˜ˆ: {\"2\": \"...\", \"3\": \"...\"})ì— ëŒ€í•´,\n",
        "    ê´€ë ¨ëœ ì„¸ë¶€ ëª©í‘œ(subgoal)ë“¤ì„ ì¶”ë¡ í•˜ê³  JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    selected_sdgs_goal_dict = json.loads(selected_sdgs_goal)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "ë„ˆëŠ” SDGs ì„¸ë¶€ ëª©í‘œ ë¶„ì„ê°€ì•¼.\n",
        "\n",
        "ì‚¬ìš©ìëŠ” ë‹¤ìŒê³¼ ê°™ì€ SDGs ëŒ€ëª©í‘œë¥¼ ì„ íƒí–ˆì–´:\n",
        "{selected_sdgs_goal_dict.keys()}\n",
        "\n",
        "ì•„ë˜ëŠ” ê° SDGs ëŒ€ëª©í‘œì— ëŒ€í•œ ì„¸ë¶€ ëª©í‘œë“¤ì´ì•¼:\n",
        "{SDGS_SUBGOALS}\n",
        "\n",
        "ê° SDGs ëŒ€ëª©í‘œì— ëŒ€í•´ ê°€ì¥ ê´€ë ¨ ìˆëŠ” **ì„¸ë¶€ ëª©í‘œ ì½”ë“œ(ì˜ˆ: 2.1, 3.2 ë“±)**ì™€ ê·¸ ì´ìœ ë¥¼ 1~2ê°œì”© JSON í˜•ì‹ìœ¼ë¡œ ì•Œë ¤ì¤˜.\n",
        "\n",
        "í˜•ì‹ ì˜ˆì‹œ:\n",
        "{{\n",
        "  \"2.1\": \"ì˜ì–‘ ê³µê¸‰ ê°œì„  â€” ì•„ë™ ê¸‰ì‹ê³¼ ê´€ë ¨\",\n",
        "  \"3.4\": \"ì •ì‹  ê±´ê°• ì¦ì§„ â€” ìš°ìš¸ì¦ ì˜ˆë°©ê³¼ ê´€ë ¨\"\n",
        "}}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt,seed=23).content\n",
        "    try:\n",
        "        # GPTê°€ ```json ... ``` ë¡œ ê°ìŒŒì„ ê°€ëŠ¥ì„± ëŒ€ë¹„\n",
        "        cleaned = re.sub(r\"```json|```\", \"\", response).strip()\n",
        "        return json.loads(cleaned)\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨. ì›ë³¸ ì‘ë‹µ:\", response)\n",
        "        return {\"error\": \"JSON parsing failed\", \"raw_response\": response}\n",
        "\n",
        "\n",
        "#=====================================================#\n",
        "#=============== Step5-2 : RAW ë³´ê³ ì„œ ì‘ì„±==============#\n",
        "#=====================================================#\n",
        "\n",
        "\n",
        "def generate_raw_report(company_info: str, sdgs_code: dict) -> str:\n",
        "    print('='*100)\n",
        "    print(\"Step5-2 :ì‚¬íšŒì  ê°€ì¹˜ ì°½ì¶œ í¬ì¸íŠ¸ ì‘ì„±\")\n",
        "    \"\"\"\n",
        "    ì†Œì…œë²¤ì²˜ ê¸°ì—… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ë„ìš°ë¯¸ë¡œ,\n",
        "    ì œê³µëœ ê¸°ì—… ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ì‚¬íšŒì  ê°€ì¹˜ ì°½ì¶œ í¬ì¸íŠ¸' ì„¹ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    ë„ˆëŠ” ì‚¬íšŒì  ê°€ì¹˜ ë¶„ì„ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì•¼.\n",
        "    ì´ì œ ë‹¤ìŒ ê¸°ì—…ì˜ ì‚¬íšŒ ê°€ì¹˜ ì°½ì¶œ ë³´ê³ ì„œì— ë“¤ì–´ê°ˆ ì‚¬íšŒì  ê°€ì¹˜ ë³´ê³ ì„œ ë‚´ìš©ì„ ë‹¤ìŒ í˜•ì‹ì— ë§ì¶° ì‘ì„±í•´ì¤˜.\n",
        "\n",
        "    ğŸ“„ ê¸°ì—… ì •ë³´:\n",
        "    {company_info}\n",
        "\n",
        "    ğŸ“Œ ì‘ì„± ì§€ì¹¨:\n",
        "    - ì „ì²´ 5ë¬¸ë‹¨ êµ¬ì„±(\"ì‚¬íšŒì  ê°€ì¹˜ ì°½ì¶œ í¬ì¸íŠ¸\", \"WHAT\", \"WHO\", \"HOW MUCH\", \"RISK\"ë¡œ ì´ 5ê°œì˜ ë¬¸ë‹¨)\n",
        "\n",
        "    1. **ì‚¬íšŒì  ê°€ì¹˜ ì°½ì¶œ í¬ì¸íŠ¸**\n",
        "    - ê¸°ì—…ì´ í•´ê²°í•˜ê³ ì í•˜ëŠ” ì‚¬íšŒë¬¸ì œì™€, ì œê³µí•˜ëŠ” ì†”ë£¨ì…˜ì˜ í•µì‹¬ì„ ë¶„ì„í•´.\n",
        "    - ì†”ë£¨ì…˜ì´ í™•ì¥ë  ê²½ìš° ê¸°ëŒ€ë˜ëŠ” ì‚¬íšŒì  íš¨ê³¼ê¹Œì§€ í¬í•¨í•´ì„œ ì‘ì„±í•´.\n",
        "    - ì˜ˆ: \"[ê¸°ì—…]ì˜ ê°€ì¹˜ëŠ” [ë¬¸ì œ]ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ [ì†”ë£¨ì…˜]ì„ ì œê³µí•¨ìœ¼ë¡œì¨ ì°½ì¶œë©ë‹ˆë‹¤. ì´ëŠ” [ì‚¬íšŒì  íš¨ê³¼]íš¨ê³¼ì˜ ì¦ëŒ€ë¡œ ì´ì–´ì§ˆ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "\n",
        "    2. **WHAT**,\n",
        "    - ì´ ê¸°ì—…ì´ ì°½ì¶œí•˜ëŠ” ê¸ì •ì ì¸ ë³€í™”(ì‚¬íšŒì  ê°€ì¹˜)ì˜ ìœ í˜•ì„ ìì„¸íˆ ì„¤ëª…í•˜ê³ , SDG {json.dumps(sdgs_code, ensure_ascii=False)}ë²ˆê³¼ ì—°ê²°ì§€ì–´ ë§ˆë¬´ë¦¬í•´.\n",
        "    - í™˜ê²½, êµìœ¡, ê±´ê°•, ê³ ìš© ë“± ì–´ë–¤ ì˜ì—­ì—ì„œ ì–´ë–¤ ê°€ì¹˜ê°€ ë°œìƒí•˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´.\n",
        "    - ì˜ˆ: â€[ê¸°ì—…]ëŠ” [ë¬¸ì œ]ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ [ê¸°ìˆ /ì„œë¹„ìŠ¤]ë¥¼ ì œê³µí•˜ë©°, ì´ëŠ” [ëŒ€ìƒ]ì˜ [ì‚¬íšŒì  íš¨ê³¼]ì— ê¸°ì—¬í•©ë‹ˆë‹¤. ì´ëŠ” SDGsì˜ [ê´€ë ¨ ëª©í‘œ] ë‹¬ì„±ê³¼ë„ ì—°ê²°ë©ë‹ˆë‹¤.â€\n",
        "\n",
        "\n",
        "    3. **WHO**\n",
        "    - í•´ë‹¹ ì‚¬íšŒì  ê°€ì¹˜ê°€ **ëˆ„êµ¬ì—ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€** ì„¤ëª…í•´.\n",
        "    - ëŒ€ìƒ ì§‘ë‹¨ì˜ íŠ¹ì„±(ì˜ˆ: ì†Œì™¸ê³„ì¸µ, ì €ì†Œë“ì¸µ, ì²­ë…„, ì§€ì—­ì‚¬íšŒ ë“±)ê³¼ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ í˜œíƒì„ ë°›ëŠ”ì§€ ëª…í™•íˆ ì‘ì„±í•´.\n",
        "    - ì˜ˆ: â€[ê¸°ì—…]ëŠ” [ëŒ€ìƒ]ì´ ê²ªëŠ” [ë¬¸ì œ]ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ [ì†”ë£¨ì…˜/ì„œë¹„ìŠ¤]ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, [ë¬¸ì œì˜ êµ¬ì²´ì  ìƒí™©]ì— ëŒ€ì‘í•˜ì—¬ [ì‹¤í–‰ ë°©ì‹]ì„ í†µí•´ [ê¸°ëŒ€ íš¨ê³¼ ë˜ëŠ” ë³€í™”]ë¥¼ ì´ëŒì–´ëƒ…ë‹ˆë‹¤. ì´ëŠ” [ëŒ€ìƒ]ì˜ ì‚¶ì˜ ì§ˆ í–¥ìƒê³¼ í•¨ê»˜ [ì‚¬íšŒì  ê°€ì¹˜] ì°½ì¶œë¡œ ì´ì–´ì§€ë©°, ì§€ì†ê°€ëŠ¥í•œ ì‚¬íšŒ êµ¬í˜„ì— ê¸°ì—¬í•©ë‹ˆë‹¤.â€\n",
        "\n",
        "\n",
        "    4. **HOW MUCH**\n",
        "    - í•´ë‹¹ ê¸°ì—…ì˜ ì„íŒ©íŠ¸(ì‚¬íšŒì  ê°€ì¹˜)ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì • ë˜ëŠ” ì‚°ì¶œí•  ìˆ˜ ìˆëŠ” ë°©ì‹ì„ ì œì‹œí•˜ë˜, ì—†ëŠ” ìˆ«ìë¥¼ ì§€ì–´ë‚´ì„œëŠ” ì•ˆë¼.\n",
        "    - ì§€í‘œê°€ ëª…í™•í•˜ì§€ ì•Šë‹¤ë©´ ê·œëª¨Â·ë²”ìœ„Â·ì§€ì†ì„±ì— ëŒ€í•œ ì¶”ì •ë„ ê°€ëŠ¥í•´.\n",
        "    - ì˜ˆ: â€[ê¸°ì—…]ì˜ ì„íŒ©íŠ¸ëŠ” [ì†”ë£¨ì…˜/ì„œë¹„ìŠ¤] ë„ì… ì „í›„ì˜ ë³€í™”ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•¨ìœ¼ë¡œì¨ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” [ì„±ê³¼ ì¸¡ì • ëŒ€ìƒ]ì— ëŒ€í•´ [ë¹„êµ ì§€í‘œ]ë¥¼ ì‚°ì¶œí•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ [ì‚¬íšŒì  ì„±ê³¼]ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.\"\n",
        "\n",
        "    5. **RISK**\n",
        "    - ì‚¬íšŒì  ê°€ì¹˜ê°€ ì‹¤ì œë¡œ ì‹¤í˜„ë˜ì§€ ëª»í•  **ê°€ëŠ¥ì„± ë˜ëŠ” ì œí•œì‚¬í•­**ì„ ì„¤ëª…í•´.\n",
        "    - ì™¸ë¶€ ìš”ì¸, ì •ì±… ë³€í™”, ì´í•´ê´€ê³„ì ì´ìŠˆ ë“± ë¦¬ìŠ¤í¬ ìš”ì¸ê³¼ ê·¸ ì˜í–¥ì— ëŒ€í•´ ë¶„ì„í•´.\n",
        "    - í•„ìš”ì‹œ ë¦¬ìŠ¤í¬ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì „ëµì´ë‚˜ ë°©ì–´ ìš”ì†Œë„ í¬í•¨í•´.\n",
        "    - ì˜ˆ: â€œ[ê¸°ì—…]ì˜ ì‚¬íšŒì  ê°€ì¹˜ ì‹¤í˜„ì—ëŠ” [ì™¸ë¶€ ìš”ì¸]ì´ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, [êµ¬ì²´ì  ìœ„í—˜ ìƒí™©]ì´ ë°œìƒí•˜ë©´ [ì„±ê³¼ ì¶•ì†Œ ë˜ëŠ” ì‚¬ì—… ì°¨ì§ˆ]ì´ ìš°ë ¤ë©ë‹ˆë‹¤. ì´ë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ [ë¦¬ìŠ¤í¬ ëŒ€ì‘ ì „ëµ]ì„ ë³‘í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤.â€\n",
        "\n",
        "\n",
        "    ğŸ§¾ ì¶œë ¥ í˜•ì‹:\n",
        "    ì‹¤ì œ ë³´ê³ ì„œ ë¬¸ë‹¨ í˜•íƒœë¡œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì‘ì„±í•´.\n",
        "    ê° ë¬¸ë‹¨ì€ ì œëª© ìˆê²Œ ìˆœì°¨ì ìœ¼ë¡œ ì´ì–´ì§€ë„ë¡ ì‘ì„±í•´.\n",
        "    \"\"\"\n",
        "\n",
        "    result = llm.invoke(prompt,seed=23).content\n",
        "    return result\n",
        "\n",
        "#=====================================================#\n",
        "#============ Step6 :ì‚¬ì‹¤ê²€ì¦ ë° ì¡°ê±´ë¶€ í•´ì„¤ ì¶”ê°€ ===========#\n",
        "#=====================================================#\n",
        "\n",
        "@tool\n",
        "def verify_and_correct_facts(paragraph: str) -> str:\n",
        "    \"\"\"\n",
        "    ë‹¹ì‹ ì€ ESG ë³´ê³ ì„œì˜ ì‚¬ì‹¤ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
        "    ë‹¤ìŒ ë¬¸ë‹¨ì´ ESG(í™˜ê²½, ì‚¬íšŒ, ì§€ë°°êµ¬ì¡°) ê´€ë ¨ ë‚´ìš©ì¸ì§€ ìš°ì„  íŒë‹¨í•˜ì„¸ìš”.\n",
        "    ESG ê´€ë ¨ ë‚´ìš©ì¼ ê²½ìš°ì—ë§Œ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ ì—„ë°€íˆ ê²€ì¦í•©ë‹ˆë‹¤.\n",
        "    ESGì™€ ì§ì ‘ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´ ìˆ˜ì •í•˜ê±°ë‚˜ í•´ì„¤í•˜ì§€ ë§ê³ , ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n",
        "    \"\"\"\n",
        "    print('ê²€ì¦ tool ì‘ë™.')\n",
        "    prompt = f\"\"\"\n",
        "    ë„ˆëŠ” ESG ë³´ê³ ì„œì˜ ì‚¬ì‹¤ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
        "    ë‹¤ìŒ ë¬¸ë‹¨ì˜ ë‚´ìš©ì„ ê²€ì¦í•˜ì„¸ìš”.\n",
        "\n",
        "    ê·œì¹™:\n",
        "    1. ë³´ê³ ì„œì— ì–¸ê¸‰ëœ ìˆ˜ì¹˜, ê¸°ê°„, ì¥ì†Œ, ê¸°ê´€ëª…, ì‚¬ê±´ ë“± ESG ê´€ë ¨ ì‚¬ì‹¤ ìš”ì†Œë¥¼ ì¤‘ì ì ìœ¼ë¡œ ê²€í† í•©ë‹ˆë‹¤.\n",
        "    2. ì‚¬ì‹¤ì´ ì •í™•í•˜ë©´ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "    3. ë¶€ì •í™•í•œ ESG ì‚¬ì‹¤ì´ ìˆìœ¼ë©´, ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì—ì„œ ì˜¬ë°”ë¥¸ ë‚´ìš©ìœ¼ë¡œ ìˆ˜ì •í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "    4. ê²€ì¦ ì‹œ ì¸í„°ë„· ì§€ì‹ê³¼ ì¼ë°˜ ìƒì‹ì„ í™œìš©í•˜ë˜, ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì¶”ì •í•˜ì§€ ë§ê³  ì›ë¬¸ ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.\n",
        "    5. ìµœì¢… ì¶œë ¥ì€ ë¬¸ë‹¨ í•œ ê°œë¡œ í•˜ë©°, ì¶”ê°€ ì„¤ëª…ì´ë‚˜ í•´ì„¤ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
        "\n",
        "    ë¬¸ë‹¨: \"{paragraph}\"\n",
        "    \"\"\"\n",
        "    result = llm.invoke(prompt,seed=23).content.strip()\n",
        "    return result\n",
        "\n",
        "@tool\n",
        "def interpret_esg_impact(statement: str) -> str:\n",
        "    \"\"\"\n",
        "    ESG ì„±ê³¼ë¥¼ ì§ê´€ì ì¸ ë¹„ìœ ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ê²½ìš°ì—ëŠ” ë¹„ìœ ë¥¼ ë¶™ì´ê³ ,\n",
        "    ë¹„ìœ ê°€ ì í•©í•˜ì§€ ì•Šìœ¼ë©´ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print('ë¹„ìœ  tool ì‘ë™.')\n",
        "\n",
        "    sdgs_to_esg = {\n",
        "        \"1\": \"S\", \"2\": \"S\", \"3\": \"S\", \"4\": \"S\", \"5\": \"S\",\n",
        "        \"6\": \"S\", \"7\": \"S\", \"8\": \"S\", \"9\": \"S\", \"10\": \"S\",\n",
        "        \"11\": \"S\", \"12\": \"E\", \"13\": \"E\", \"14\": \"E\", \"15\": \"E\",\n",
        "        \"16\": \"G\", \"17\": \"G\"\n",
        "    }\n",
        "\n",
        "    guide_dict = {\n",
        "        \"E\": \"í™˜ê²½ ë³´í˜¸ ë˜ëŠ” ìì› ì ˆê°ê³¼ ê´€ë ¨ë¨. íƒ„ì†Œì €ê°, í”Œë¼ìŠ¤í‹± ì ˆê°, ì—ë„ˆì§€ íš¨ìœ¨ ë“±ì— ëŒ€í•œ ìˆ˜ì¹˜ì Â·ë¹„ìœ ì  ì„¤ëª… ê°€ëŠ¥ ì—¬ë¶€ë¥¼ íŒë‹¨.\",\n",
        "        \"S\": \"ì‚¬íšŒì  ê°€ì¹˜, ê³ ìš©, êµìœ¡, ê±´ê°•, í¬ìš©ì„±ê³¼ ê´€ë ¨ë¨. ì‚¬ëŒë“¤ì˜ ì‚¶ì— ì‹¤ì§ˆì  ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ë¹„ìœ  ê°€ëŠ¥ ì—¬ë¶€ë¥¼ íŒë‹¨.\",\n",
        "        \"G\": \"ê¸°ì—… ì§€ë°°êµ¬ì¡°, ìœ¤ë¦¬ì„±, íˆ¬ëª…ì„±ê³¼ ê´€ë ¨ë¨. ì‹ ë¢°Â·íˆ¬ëª…ì„±ì— ê¸°ì—¬í•˜ëŠ” íš¨ê³¼ë¥¼ ë¹„ìœ  ê°€ëŠ¥ ì—¬ë¶€ë¥¼ íŒë‹¨.\",\n",
        "        \"\":  \"ESG ì¹´í…Œê³ ë¦¬ ë¶ˆë¶„ëª…. ê·¸ë˜ë„ ì§ê´€ì  ì„¤ëª… ê°€ëŠ¥ ì—¬ë¶€ë¥¼ íŒë‹¨.\"\n",
        "    }\n",
        "\n",
        "    # 1ì°¨ í”„ë¡¬í”„íŠ¸: ë¹„ìœ  ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨\n",
        "    check_prompt = f\"\"\"\n",
        "    ë‹¤ìŒ ì„±ê³¼ ë¬¸ì¥ì´ ì¼ë°˜ ëŒ€ì¤‘ì´ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” **ì‹¤ìƒí™œ ë¹„ìœ **ë¡œ í‘œí˜„ ê°€ëŠ¥í•œì§€ íŒë‹¨í•˜ì„¸ìš”.\n",
        "    - ê°€ëŠ¥í•˜ë‹¤ë©´ \"YES\"ë§Œ ì¶œë ¥\n",
        "    - ì–´ë µê±°ë‚˜ ë¶€ì í•©í•˜ë©´ \"NO\"ë§Œ ì¶œë ¥\n",
        "    ë¬¸ì¥: \"{statement}\"\n",
        "    \"\"\"\n",
        "    can_metaphor = llm.invoke(check_prompt).content.strip().upper()\n",
        "\n",
        "    if can_metaphor != \"YES\":\n",
        "        # ë¹„ìœ  ë¶ˆê°€ â†’ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
        "        return statement\n",
        "\n",
        "    # 2ì°¨ í”„ë¡¬í”„íŠ¸: ë¹„ìœ  ìƒì„±\n",
        "    prompt = f\"\"\"\n",
        "    ë„ˆëŠ” ESG ì„±ê³¼ë¥¼ ì¼ë°˜ ëŒ€ì¤‘ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
        "\n",
        "    ì¡°ê±´:\n",
        "    - ë³µì¡í•œ í‘œí˜„ì€ í”¼í•˜ê³ ,\n",
        "    - ì¼ë°˜ ëŒ€ì¤‘ì´ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ **ì‹¤ìƒí™œ ë¹„ìœ ** ë˜ëŠ” **ìˆ˜ì¹˜ì  í™˜ì‚°** í¬í•¨\n",
        "    - ë³´ê³ ì„œì— ëª…ì‹œëœ ìˆ˜ì¹˜ë§Œ ì‚¬ìš© (ì—†ëŠ” ê²½ìš° ì •ëŸ‰ í‘œí˜„ ìƒëµ)\n",
        "    - ë¬¸ì¥ì€ 1~2ì¤„\n",
        "    - ê³¼ë„í•œ ê°ì„± í‘œí˜„ ê¸ˆì§€\n",
        "\n",
        "\n",
        "    ë¬¸ì¥: \"{statement}\"\n",
        "\n",
        "    ì¶œë ¥ ì˜ˆì‹œ:\n",
        "    - \"ì´ëŠ” 30ë…„ìƒ ì†Œë‚˜ë¬´ ì•½ 100ë§Œ ê·¸ë£¨ë¥¼ ì‹¬ì€ íš¨ê³¼ì™€ ê°™ìŠµë‹ˆë‹¤.\"\n",
        "    - \"ì´ëŠ” í•œ ë§ˆì„ì˜ ëª¨ë“  ê°€ì •ì— ì•ˆì •ì ì¸ ì¼ìë¦¬ë¥¼ ì œê³µí•œ ì…ˆì…ë‹ˆë‹¤.\"\n",
        "    \"\"\"\n",
        "    result = llm.invoke(prompt,seed=23).content\n",
        "    statement_with_explanation = statement + '\\n\\nAIí•´ì„¤: ' + result\n",
        "    return statement_with_explanation\n",
        "\n",
        "agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=[interpret_esg_impact, verify_and_correct_facts],\n",
        "        prompt=\"\"\"\n",
        "        ë„ˆëŠ” ESG ë³´ê³ ì„œ ë¶„ì„ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ì•¼.\n",
        "        ë‘ ê°€ì§€ ì—­í• ì„ ìˆ˜í–‰í•´ì•¼ í•´:\n",
        "        1) ESG ë¬¸ë‹¨ì˜ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ ê²€ì¦í•˜ê³ , í•„ìš”ì‹œ ì˜¬ë°”ë¥´ê²Œ ìˆ˜ì •í•´ì¤˜.\n",
        "        2) ì‚¬ì‹¤ì´ ê²€ì¦ëœ ë¬¸ë‹¨ì—ì„œ, ë¹„ìœ ê°€ ê°€ëŠ¥í•˜ë‹¤ë©´, ëŒ€ì¤‘ì´ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‹¤ìƒí™œ ë¹„ìœ ë‚˜ ì§ê´€ì  ì„¤ëª…ì„ 1~2ì¤„ë¡œ ë§ë¶™ì—¬ì¤˜. ë‹¨, ë¹„ìœ ê°€ ì ì ˆí•˜ì§€ ì•Šìœ¼ë©´ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•´ì•¼ í•´.\n",
        "        í•­ìƒ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ë‹µí•´.\n",
        "        íˆ´ì„ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©í•´ì„œ ë¬¸ë‹¨ë³„ë¡œ ì²˜ë¦¬í•´ì¤˜.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "\n",
        "#=====================================================#\n",
        "#Step6 :ì‚¬íšŒì  ê°€ì¹˜ ì°½ì¶œ í¬ì¸íŠ¸ì— ëŒ€í•œ ì‚¬ì‹¤ê²€ì¦ ë° ì¡°ê±´ë¶€ í•´ì„¤ ì¶”ê°€#\n",
        "#=====================================================#\n",
        "\n",
        "def process_raw_report_agent(report_text: str) -> dict:\n",
        "    print('='*100)\n",
        "    print(\"Step6 :ì‚¬íšŒì  ê°€ì¹˜ ì°½ì¶œ í¬ì¸íŠ¸ì— ëŒ€í•œ ì‚¬ì‹¤ê²€ì¦ ë° ì¡°ê±´ë¶€ í•´ì„¤ ì¶”ê°€\")\n",
        "    paragraphs = [p.strip() for p in report_text.split(\"\\n\\n\") if p.strip()]\n",
        "\n",
        "    paragraph = paragraphs[0]\n",
        "    # 1ë‹¨ê³„: ì‚¬ì‹¤ê²€ì¦ ë° ìˆ˜ì •\n",
        "    verify_instruction = (\n",
        "        \"ë‹¹ì‹ ì€ ESG ë³´ê³ ì„œ ë¬¸ë‹¨ì˜ ì‚¬ì‹¤ì„ ê²€ì¦í•˜ëŠ” ì—­í• ì„ ë§¡ì•˜ìŠµë‹ˆë‹¤.\\n\"\n",
        "        \"ì•„ë˜ ë¬¸ë‹¨ì´ ì‚¬ì‹¤ì— ë¶€í•©í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ë¶€ì •í™•í•˜ë©´ ì •í™•í•˜ê²Œ ìˆ˜ì •í•´ ì£¼ì„¸ìš”.\\n\"\n",
        "        \"ìˆ˜ì •í•˜ì§€ ì•Šì•„ë„ ë˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•´ ì£¼ì„¸ìš”.\\n\"\n",
        "        \"ë‹¤ë¥¸ ì„¤ëª…, ë ˆì´ë¸”, ìˆ˜ì‹ì–´, ì´ìœ ëŠ” ì ˆëŒ€ ë¶™ì´ì§€ ë§ê³ , ìˆ˜ì •ëœ ë¬¸ë‹¨ ë‚´ìš©ë§Œ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”.\\n\"\n",
        "        \"ì˜ëª»ëœ ì˜ˆì‹œ: ìˆ˜ì •ëœ ë¬¸ì¥: ì´ ê¸°ì—…ì€...\\n\"\n",
        "        \"ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: ì´ ê¸°ì—…ì€...\\n\\n\"\n",
        "        \"ë¬¸ë‹¨:\\n\" + paragraph\n",
        "    )\n",
        "    verify_response = agent.invoke(\n",
        "        {\"messages\": [HumanMessage(content=verify_instruction)]}\n",
        "    )\n",
        "\n",
        "    messages = verify_response.get('messages', [])\n",
        "    if messages:\n",
        "        last_message = messages[-1]\n",
        "        corrected_paragraph = last_message.content.strip()\n",
        "    else:\n",
        "        corrected_paragraph = paragraph\n",
        "\n",
        "\n",
        "    # 2ë‹¨ê³„: ë¹„ìœ  ìƒì„±\n",
        "    interpret_instruction = (\n",
        "        \"ë‹¹ì‹ ì€ ESG ì„±ê³¼ë¥¼ ëŒ€ì¤‘ì´ ì‰½ê²Œ ì´í•´í•˜ë„ë¡ ë¹„ìœ í•˜ëŠ” ë¬¸ì¥ì„ ìƒì„±í•  ìˆ˜ ìˆì„ì§€ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.\\n\"\n",
        "        \"ë¹„ìœ ê°€ ê°€ëŠ¥í•˜ë©´ **ë¹„ìœ  ë¬¸ì¥ë§Œ** ë°˜í™˜í•˜ì„¸ìš”.\\n\"\n",
        "        \"ë¹„ìœ ê°€ ë¶ˆê°€ëŠ¥í•˜ë©´ **Noneì„** ë°˜í™˜í•˜ì„¸ìš”.\\n\"\n",
        "        \"ë¹„ìœ  ë¬¸ì¥ì€ ë¶ˆí•„ìš”í•œ ì„œë‘ë‚˜ ì´ìœ  ì„¤ëª… ì—†ì´ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\\n\"\n",
        "        \"ì›ë¬¸ ë¬¸ë‹¨ì€ ì ˆëŒ€ ìˆ˜ì •í•˜ê±°ë‚˜ ì¬ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.\\n\\n\"\n",
        "\n",
        "        \"ì˜ˆì‹œ 1:\\n\"\n",
        "        \"ë¬¸ì¥: ì´ ê¸°ì—…ì€ ì§€ë‚œ 1ë…„ê°„ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ì„ 25% ê°ì¶•í•˜ì—¬ í™˜ê²½ ë³´í˜¸ì— ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.\\n\"\n",
        "        \"ë¹„ìœ : ì´ëŠ” 5,000ê°€êµ¬ê°€ í•œ í•´ ë™ì•ˆ ì‚¬ìš©í•˜ëŠ” ì „ê¸°ë¥¼ ì ˆê°í•œ ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.\\n\\n\"\n",
        "\n",
        "        \"ì˜ˆì‹œ 2:\\n\"\n",
        "        \"ë¬¸ì¥: ì´ íšŒì‚¬ëŠ” ì§ì› ë‹¤ì–‘ì„±ê³¼ í¬ìš©ì„±ì„ ì¦ì§„í•˜ê¸° ìœ„í•œ ì •ì±…ì„ ê°•í™”í–ˆìŠµë‹ˆë‹¤.\\n\"\n",
        "        \"ë¹„ìœ : None\\n\\n\"  # ë¹„ìœ  ë¶ˆê°€ ì‹œ None ë°˜í™˜ ì˜ˆì‹œ\n",
        "\n",
        "        \"ì˜ˆì‹œ 3:\\n\"\n",
        "        \"ë¬¸ì¥: ë³¸ ê¸°ì—…ì˜ ì‚¬íšŒê³µí—Œ í”„ë¡œê·¸ë¨ì€ ì§€ì—­ì‚¬íšŒ êµìœ¡ ì§€ì›ì— ì§‘ì¤‘í•˜ì—¬ ìˆ˜ë°± ëª…ì˜ í•™ìƒë“¤ì—ê²Œ ì–‘ì§ˆì˜ êµìœ¡ ê¸°íšŒë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤.\\n\"\n",
        "        \"ë¹„ìœ : ì´ëŠ” í•œ ë„ì‹œì˜ ëª¨ë“  ì´ˆë“±í•™êµ í•™ìƒë“¤ì—ê²Œ ë§ì¶¤í˜• êµìœ¡ì„ ì œê³µí•œ íš¨ê³¼ì™€ ê°™ìŠµë‹ˆë‹¤.\\n\\n\"\n",
        "\n",
        "        f\"ë¬¸ë‹¨:\\n{corrected_paragraph}\"\n",
        "    )\n",
        "\n",
        "\n",
        "    interpret_response = agent.invoke(\n",
        "        {\"messages\": [HumanMessage(content=interpret_instruction)]}\n",
        "    )\n",
        "\n",
        "    messages = interpret_response.get('messages', [])\n",
        "    if messages:\n",
        "        last_message = messages[-1]\n",
        "        ai_explain = last_message.content.strip()\n",
        "    else:\n",
        "        ai_explain = corrected_paragraph.strip()\n",
        "\n",
        "    print('ai_explain:')\n",
        "    print(ai_explain)\n",
        "\n",
        "    paragraphs[0] = corrected_paragraph\n",
        "    return ai_explain, paragraphs\n",
        "\n",
        "def summarize_validated_report(validated_report: str, selected_sdgs_code: str) -> str:\n",
        "    sdgs_main_code = json.loads(selected_sdgs_code)\n",
        "    sdgs_main_code_str = ','.join(map(str, sdgs_main_code.keys()))\n",
        "\n",
        "    prompt = (\n",
        "        \"ì œê³µëœ ESG ë³´ê³ ì„œ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê³  ìì„¸íˆ ì„¤ëª…í•˜ëŠ” ë¬¸ë‹¨ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.\\n\"\n",
        "        f\"ê¸°ì—… ì†Œê°œ, SDG ë²ˆí˜¸ {sdgs_main_code_str}, ê·¸ë¦¬ê³  ì‚¬íšŒì  ê°€ì¹˜ì— ëŒ€í•´ ì—°ì†ì ìœ¼ë¡œ ì„œìˆ í•´ ì£¼ì„¸ìš”.\\n\"\n",
        "        \"ë‹¤ìŒ ì–‘ì‹ì— ë§ì¶°ì„œ, ë¶ˆí•„ìš”í•œ ë‚´ìš©ì€ ì œì™¸í•˜ê³  í•µì‹¬ë§Œ ëª…í™•í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\\n\\n\"\n",
        "        \"ì–‘ì‹:\\n\"\n",
        "        \"[ê¸°ì—…]ì€ [ESG ì‚°ì—… ë¶„ì•¼] ì‚°ì—…ì—ì„œ ì„ ë„ì ì¸ ì—­í• ì„ í•˜ëŠ” ê¸°ì—…ìœ¼ë¡œ, SDG {sdgs_main_code_str}ë²ˆì— ë¶€í•©í•©ë‹ˆë‹¤. \"\n",
        "        \"[ê¸°ì—…]ì€ [ì£¼ìš” í™œë™/ì „ëµ]ë¥¼ í†µí•´ [ì‚¬íšŒì /í™˜ê²½ì  ê°€ì¹˜]ì— ê¸°ì—¬í•˜ë©°, [ì¶”ê°€ í™œë™]ë¥¼ í†µí•´ ì§€ì†ê°€ëŠ¥í•œ ë¯¸ë˜ë¥¼ ì¶”êµ¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.\\n\\n\"\n",
        "        f\"{validated_report}\\n\"\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(prompt, seed=23)\n",
        "    return response.content.strip()\n",
        "\n",
        "def to_json(company_summary, sdgs_main, sdgs_sub, gri_main, ai_explain, validated_report_list, ai_summary):\n",
        "    final_result = {\n",
        "        'company_summary': company_summary,\n",
        "        'sdgs_main': json.loads(sdgs_main),\n",
        "        'sdgs_sub': sdgs_sub,\n",
        "        'gri_main': gri_main  # <-- ì—¬ê¸°ì— GRI ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ë©ë‹ˆë‹¤.\n",
        "    }\n",
        "\n",
        "    paragraphs = validated_report_list\n",
        "\n",
        "    def safe_extract(index, keyword):\n",
        "        try:\n",
        "            return paragraphs[index].split(keyword, 1)[-1].strip()\n",
        "        except Exception:\n",
        "            return paragraphs[index] if index < len(paragraphs) else \"\"\n",
        "\n",
        "    final_result['social_value_point'] = safe_extract(0, 'ì‚¬íšŒì  ê°€ì¹˜ ì°½ì¶œ í¬ì¸íŠ¸')\n",
        "    final_result['ai_explain'] = ai_explain if ai_explain and ai_explain.strip().upper() != 'NONE' else 'None'\n",
        "    final_result['what'] = safe_extract(1, 'WHAT')\n",
        "    final_result['who'] = safe_extract(2, 'WHO')\n",
        "    final_result['how_much'] = safe_extract(3, 'HOW MUCH')\n",
        "    final_result['risk'] = safe_extract(4, 'RISK')\n",
        "    final_result['ai_summary'] = ai_summary\n",
        "\n",
        "    return final_result\n",
        "\n",
        "# =================================================================\n",
        "# ì„¹ì…˜ B: GRI ë¶„ì„ ê´€ë ¨ í•¨ìˆ˜ë“¤\n",
        "# =================================================================\n",
        "\n",
        "# GRI ìƒìœ„ í•­ëª©ì— ëŒ€í•œ ëª…í™•í•œ 'ì •ì˜'ë¥¼ ì¶”ê°€í•˜ì—¬ LLMì˜ í˜¼ë™ì„ ë°©ì§€í•©ë‹ˆë‹¤.\n",
        "GRI_TOP_DEFINITIONS = [\n",
        "    {\"code\": \"201\", \"name\": \"ê²½ì œ ì„±ê³¼\", \"description\": \"ì¡°ì§ì˜ ê²½ì œì  ì„±ê³¼ì™€ ê·¸ê²ƒì´ ì´í•´ê´€ê³„ìì—ê²Œ ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"202\", \"name\": \"ì‹œì¥ ì§€ìœ„\", \"description\": \"ê²½ìŸ, ë…ì ê¸ˆì§€, ê³µì • ê²½ìŸê³¼ ê´€ë ¨ëœ ì¡°ì§ì˜ í–‰ë™ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"203\", \"name\": \"ê°„ì ‘ ê²½ì œ ì˜í–¥\", \"description\": \"ì§€ì—­ ì‚¬íšŒ ê°œë°œ ë° ì¸í”„ë¼ íˆ¬ìì™€ ê°™ì€ ì¡°ì§ì˜ ê°„ì ‘ì ì¸ ê²½ì œì  ì˜í–¥ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"204\", \"name\": \"ì¡°ë‹¬ ê´€í–‰\", \"description\": \"ê³µê¸‰ì—…ì²´ ì„ ì • ë° í˜„ì§€ ì¡°ë‹¬ê³¼ ê°™ì€ ì¡°ì§ì˜ ì¡°ë‹¬ ê´€í–‰ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"205\", \"name\": \"ë°˜ë¶€íŒ¨\", \"description\": \"ë‡Œë¬¼ ë° ë¶€íŒ¨ ë°©ì§€ë¥¼ ìœ„í•œ ì¡°ì§ì˜ ì •ì±… ë° ê´€í–‰ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"206\", \"name\": \"ë°˜ê²½ìŸì  í–‰ìœ„\", \"description\": \"ê°€ê²© ë‹´í•©, ì¹´ë¥´í…” ë“± ë°˜ê²½ìŸì  í–‰ìœ„ì— ëŒ€í•œ ì¡°ì§ì˜ ë²•ì  ì¡°ì¹˜ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"301\", \"name\": \"ì›ìì¬\", \"description\": \"ì œí’ˆ ë° ì„œë¹„ìŠ¤ ìƒì‚°ì— ì‚¬ìš©ë˜ëŠ” ì›ìì¬ì˜ ë¬´ê²Œ ë˜ëŠ” ë¶€í”¼ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"302\", \"name\": \"ì—ë„ˆì§€\", \"description\": \"ì¡°ì§ ë‚´ ì—ë„ˆì§€ ì†Œë¹„ëŸ‰ ë° ì—ë„ˆì§€ íš¨ìœ¨ ê°œì„  ë…¸ë ¥ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"303\", \"name\": \"ìˆ˜ìì› ë° íìˆ˜\", \"description\": \"ì¡°ì§ì˜ ë¬¼ ì‚¬ìš©ëŸ‰, íìˆ˜ ë°°ì¶œ ë° ìˆ˜ìì› ê´€ë¦¬ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"304\", \"name\": \"ìƒë¬¼ë‹¤ì–‘ì„±\", \"description\": \"ë³´í˜¸ ì§€ì—­ ì•ˆíŒì—ì„œ ìƒë¬¼ë‹¤ì–‘ì„±ì— ë¯¸ì¹˜ëŠ” ì¡°ì§ì˜ ì˜í–¥ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"305\", \"name\": \"ì˜¨ì‹¤ê°€ìŠ¤ ë°°ì¶œ\", \"description\": \"ì¡°ì§ì˜ ì˜¨ì‹¤ê°€ìŠ¤(GHG) ì§ì ‘ ë° ê°„ì ‘ ë°°ì¶œëŸ‰ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"306\", \"name\": \"íê¸°ë¬¼\", \"description\": \"ì¡°ì§ì˜ íê¸°ë¬¼ ë°œìƒëŸ‰ ë° ì²˜ë¦¬ ë°©ë²•ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"308\", \"name\": \"ê³µê¸‰ì—…ì²´ í™˜ê²½ í‰ê°€\", \"description\": \"í™˜ê²½ ê¸°ì¤€ì„ ì‚¬ìš©í•˜ì—¬ ì‹ ê·œ ë° ê¸°ì¡´ ê³µê¸‰ì—…ì²´ë¥¼ í‰ê°€í•˜ëŠ” ì¡°ì§ì˜ ê´€í–‰ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"401\", \"name\": \"ê³ ìš©\", \"description\": \"ì¡°ì§ì˜ ì§ì› ì±„ìš©, ì´ì§, ë³µë¦¬í›„ìƒ ë“± ì§ì ‘ì ì¸ ê³ ìš© ê´€í–‰ì— ëŒ€í•œ í‘œì¤€. (ì£¼ìš” ëŒ€ìƒ: ì§ì›)\"},\n",
        "    {\"code\": \"402\", \"name\": \"ë…¸ì‚¬ ê´€ê³„\", \"description\": \"ë‹¨ì²´ êµì„­ í˜‘ì•½ì— í¬í•¨ëœ ì§ì› ë¹„ìœ¨ ë“± ì¡°ì§ì˜ ë…¸ì‚¬ ê´€ê³„ì— ëŒ€í•œ í‘œì¤€. (ì£¼ìš” ëŒ€ìƒ: ì§ì›)\"},\n",
        "    {\"code\": \"403\", \"name\": \"ì‚°ì—…ì•ˆì „ë³´ê±´\", \"description\": \"ì¡°ì§ì˜ ì‘ì—…ì¥ ë‚´ ì•ˆì „ë³´ê±´ ì‹œìŠ¤í…œ ë° ì¬í•´ìœ¨ì— ëŒ€í•œ í‘œì¤€. (ì£¼ìš” ëŒ€ìƒ: ì§ì›)\"},\n",
        "    {\"code\": \"404\", \"name\": \"í›ˆë ¨ ë° êµìœ¡\", \"description\": \"ì§ì›ì˜ ê¸°ìˆ  ê°œë°œ ë° ê²½ë ¥ ê°œë°œì„ ìœ„í•œ êµìœ¡ í”„ë¡œê·¸ë¨ì— ëŒ€í•œ í‘œì¤€. (ì£¼ìš” ëŒ€ìƒ: ì§ì›)\"},\n",
        "    {\"code\": \"405\", \"name\": \"ë‹¤ì–‘ì„± ë° ë™ë“±í•œ ê¸°íšŒ\", \"description\": \"ì¡°ì§ ë‚´ ì„±ë³„, ì—°ë ¹ ë“± ë‹¤ì–‘ì„± ë° ì°¨ë³„ ê¸ˆì§€ì— ëŒ€í•œ í‘œì¤€. (ì£¼ìš” ëŒ€ìƒ: ì§ì›)\"},\n",
        "    {\"code\": \"406\", \"name\": \"ì°¨ë³„ ê¸ˆì§€\", \"description\": \"ì°¨ë³„ ì‚¬ê±´ ë° ì‹œì • ì¡°ì¹˜ì— ëŒ€í•œ í‘œì¤€. (ì£¼ìš” ëŒ€ìƒ: ì§ì› ë° ì´í•´ê´€ê³„ì)\"},\n",
        "    {\"code\": \"407\", \"name\": \"ê²°ì‚¬ì˜ ììœ  ë° ë‹¨ì²´êµì„­ê¶Œ\", \"description\": \"ê²°ì‚¬ì˜ ììœ ì™€ ë‹¨ì²´êµì„­ê¶Œì´ ì¹¨í•´ë  ìœ„í—˜ì´ ìˆëŠ” ì‚¬ì—…ì¥ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"408\", \"name\": \"ì•„ë™ ë…¸ë™\", \"description\": \"ì•„ë™ ë…¸ë™ì˜ ìœ„í—˜ì´ ìˆëŠ” ì‚¬ì—…ì¥ ë° ì•„ë™ ë…¸ë™ ë°©ì§€ ì •ì±…ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"409\", \"name\": \"ê°•ì œ ë˜ëŠ” ì˜ë¬´ ë…¸ë™\", \"description\": \"ê°•ì œ ë˜ëŠ” ì˜ë¬´ ë…¸ë™ì˜ ìœ„í—˜ì´ ìˆëŠ” ì‚¬ì—…ì¥ ë° ê´€ë ¨ ë°©ì§€ ì •ì±…ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"412\", \"name\": \"ì¸ê¶Œ í‰ê°€\", \"description\": \"ì¸ê¶Œ ì¡°í•­ì„ í¬í•¨í•˜ëŠ” ê³„ì•½ ë° ì¸ê¶Œ í‰ê°€ë¥¼ ë°›ì€ ì‚¬ì—…ì¥ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"413\", \"name\": \"ì§€ì—­ ì‚¬íšŒ\", \"description\": \"ì§€ì—­ ì‚¬íšŒ ì°¸ì—¬, ì˜í–¥ í‰ê°€ ë° ê°œë°œ í”„ë¡œê·¸ë¨ì— ëŒ€í•œ í‘œì¤€. (ì£¼ìš” ëŒ€ìƒ: ì§€ì—­ì‚¬íšŒ)\"},\n",
        "    {\"code\": \"414\", \"name\": \"ê³µê¸‰ì—…ì²´ ì‚¬íšŒ í‰ê°€\", \"description\": \"ì‚¬íšŒì  ê¸°ì¤€ì„ ì‚¬ìš©í•˜ì—¬ ì‹ ê·œ ë° ê¸°ì¡´ ê³µê¸‰ì—…ì²´ë¥¼ í‰ê°€í•˜ëŠ” ì¡°ì§ì˜ ê´€í–‰ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"415\", \"name\": \"ê³µê³µ ì •ì±…\", \"description\": \"ë¡œë¹„ ë° ì •ì¹˜ ê¸°ë¶€ê¸ˆ ë“± ê³µê³µ ì •ì±…ì— ëŒ€í•œ ì¡°ì§ì˜ ì°¸ì—¬ì— ëŒ€í•œ í‘œì¤€.\"},\n",
        "    {\"code\": \"416\", \"name\": \"ê³ ê° ê±´ê°• ë° ì•ˆì „\", \"description\": \"ì œí’ˆ ë° ì„œë¹„ìŠ¤ê°€ ê³ ê°ì˜ ê±´ê°•ê³¼ ì•ˆì „ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë° ê´€ë ¨ í‰ê°€ì— ëŒ€í•œ í‘œì¤€. (ì£¼ìš” ëŒ€ìƒ: ê³ ê°)\"},\n",
        "    {\"code\": \"417\", \"name\": \"ë§ˆì¼€íŒ… ë° ë¼ë²¨ë§\", \"description\": \"ì œí’ˆ ì •ë³´, ë¼ë²¨ë§, ë§ˆì¼€íŒ… ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì˜ ì •í™•ì„± ë° íˆ¬ëª…ì„±ì— ëŒ€í•œ í‘œì¤€. (ì£¼ìš” ëŒ€ìƒ: ê³ ê°)\"},\n",
        "    {\"code\": \"418\", \"name\": \"ê³ ê° ê°œì¸ì •ë³´ë³´í˜¸\", \"description\": \"ê³ ê° ë°ì´í„°ì˜ ê°œì¸ì •ë³´ë³´í˜¸ ì¹¨í•´ ë° ìœ ì¶œ ê´€ë ¨ ë¶ˆë§Œ ì‚¬í•­ì— ëŒ€í•œ í‘œì¤€. (ì£¼ìš” ëŒ€ìƒ: ê³ ê°)\"}\n",
        "]\n",
        "\n",
        "# =========================\n",
        "# 2) ê³µí†µ ìœ í‹¸ ë° ë°ì´í„° êµ¬ì¡° (ì´ì „ê³¼ ë™ì¼)\n",
        "# =========================\n",
        "def load_json(path: str):\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ: {path}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"ì˜¤ë¥˜: JSON í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. íŒŒì¼: {path}\")\n",
        "        return None\n",
        "\n",
        "def normalize(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    text = re.sub(r\"[\\u200b\\ufeff]\", \"\", text)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def find_sub_items(top_item_code: str, detail_data: dict) -> list[Document]:\n",
        "    sub_items = []\n",
        "    for key, value in detail_data.items():\n",
        "        if key.startswith(top_item_code):\n",
        "            doc = Document(page_content=f\"{key}: {value}\", metadata={\"gri_code\": key})\n",
        "            sub_items.append(doc)\n",
        "    return sub_items\n",
        "\n",
        "class TopItem(BaseModel):\n",
        "    item_code: str = Field(description=\"ì„ íƒëœ ìƒìœ„ í•­ëª©ì˜ ì½”ë“œ (ì˜ˆ: '401')\")\n",
        "    item_name: str = Field(description=\"ì„ íƒëœ ìƒìœ„ í•­ëª©ì˜ ì´ë¦„ (ì˜ˆ: 'ê³ ìš©')\")\n",
        "    reason: str = Field(description=\"ì´ í•­ëª©ì„ ì„ íƒí•œ ê°„ëµí•œ ì´ìœ \")\n",
        "\n",
        "class TopItemList(BaseModel):\n",
        "    top_items: list[TopItem] = Field(description=\"ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ìƒìœ„ í•­ëª© ë¦¬ìŠ¤íŠ¸\")\n",
        "\n",
        "# =========================\n",
        "# 3) ë‹¨ê³„ë³„ ë¶„ì„ í•¨ìˆ˜\n",
        "# =========================\n",
        "# â˜…â˜…â˜…â˜…â˜… ì—¬ê¸°ê°€ í•µì‹¬ ìˆ˜ì • ì‚¬í•­ (2) â˜…â˜…â˜…â˜…â˜…\n",
        "def run_stage1_find_top_items(biz_context: str, gri_definitions: list, model_name: str, top_n: int) -> dict:\n",
        "    \"\"\"1ë‹¨ê³„: LLMì„ í†µí•´ ìƒìœ„ GRI í•­ëª©ì„ íŒë³„í•©ë‹ˆë‹¤.\"\"\"\n",
        "    print('='*100)\n",
        "    print(\"Step5-3: ìƒìœ„ GRI í•­ëª© íŒë³„\")\n",
        "    print(\"--- 1ë‹¨ê³„: ê´€ë ¨ì„± ë†’ì€ ìƒìœ„ GRI í•­ëª© íŒë³„ ì‹œì‘ ---\")\n",
        "\n",
        "    top_level_prompt_template = \"\"\"\n",
        "    ë‹¹ì‹ ì€ ìµœê³  ìˆ˜ì¤€ì˜ ESG ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì‚¬ì—… ë‚´ìš©ê³¼ GRI í‘œì¤€ ì •ì˜ ëª©ë¡ì„ ë¶„ì„í•˜ì—¬, ê°€ì¥ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ GRI ìƒìœ„ í•­ëª©ì„ {top_n}ê°œ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "    ë‹¤ìŒ 3ë‹¨ê³„ ì‚¬ê³  ê³¼ì •ì— ë”°ë¼ ë¶„ì„í•˜ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
        "\n",
        "    1.  **í•µì‹¬ ì˜í–¥ ë¶„ì„**: ì´ ì‚¬ì—…ì˜ í•µì‹¬ í™œë™ì€ ë¬´ì—‡ì´ë©°, ê·¸ë¡œ ì¸í•´ ê°€ì¥ ì§ì ‘ì ìœ¼ë¡œ ì˜í–¥ì„ ë°›ëŠ” ì£¼ìš” ì´í•´ê´€ê³„ì(ì˜ˆ: ì§ì›, ê³ ê°, ì§€ì—­ì‚¬íšŒ, í™˜ê²½)ëŠ” ëˆ„êµ¬ì…ë‹ˆê¹Œ?\n",
        "\n",
        "    2.  **GRI í‘œì¤€ ê²€í† **: ì•„ë˜ì— ì œê³µëœ GRI í‘œì¤€ ì •ì˜ ëª©ë¡ê³¼ ê·¸ ë³¸ë˜ì˜ ëª©ì ì„ ê²€í† í•©ë‹ˆë‹¤. ê° í‘œì¤€ì´ ì–´ë–¤ ì´í•´ê´€ê³„ìë¥¼ ëŒ€ìƒìœ¼ë¡œ í•˜ëŠ”ì§€ íŠ¹íˆ ì£¼ì˜ ê¹Šê²Œ ì‚´í´ë³´ì„¸ìš”.\n",
        "\n",
        "    3.  **ì •í™•í•œ ë§¤ì¹­ ë° ê·¼ê±° ì œì‹œ**: 1ë‹¨ê³„ì—ì„œ ë¶„ì„í•œ 'í•µì‹¬ ì˜í–¥'ê³¼ 'ì£¼ìš” ì´í•´ê´€ê³„ì'ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ê°€ì¥ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ GRI í•­ëª©ì„ ì„ íƒí•©ë‹ˆë‹¤. ì„ íƒí•œ ì´ìœ ë¥¼ ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ê³ , ê°„ì ‘ì ì´ê±°ë‚˜ ë¹„ìœ ì ì¸ ì—°ê²°ì€ í”¼í•´ì£¼ì„¸ìš”.\n",
        "\n",
        "    ### ë¶„ì„í•  ê¸°ì—… í™œë™ ë‚´ìš©:\n",
        "    {business_context}\n",
        "\n",
        "    ### GRI í‘œì¤€ ì •ì˜ ëª©ë¡:\n",
        "    {gri_items}\n",
        "\n",
        "    ### ì¶œë ¥ í˜•ì‹:\n",
        "    ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ì— ë§ì¶°ì„œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
        "    {format_instructions}\n",
        "    \"\"\"\n",
        "\n",
        "    parser = JsonOutputParser(pydantic_object=TopItemList)\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        template=top_level_prompt_template,\n",
        "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        "    )\n",
        "    llm = ChatOpenAI(model=model_name, temperature=0)\n",
        "    chain = prompt | llm | parser\n",
        "\n",
        "    # GRI ì •ì˜ ëª©ë¡ì„ í”„ë¡¬í”„íŠ¸ì— ë§ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
        "    gri_list_str = \"\\n\".join([f\"- GRI {item['code']} {item['name']}: {item['description']}\" for item in gri_definitions])\n",
        "\n",
        "    result = chain.invoke({\n",
        "        \"top_n\": TOPN_SELECT,\n",
        "        \"gri_items\": gri_list_str,\n",
        "        \"business_context\": biz_context\n",
        "    })\n",
        "    print(\"--- 1ë‹¨ê³„ íŒë³„ ì™„ë£Œ ---\")\n",
        "    return result\n",
        "\n",
        "def run_stage2_find_sub_item(top_item: dict, gri_detail_data: dict, biz_context: str, embeddings: OpenAIEmbeddings) -> dict | None:\n",
        "    \"\"\"2ë‹¨ê³„: íŠ¹ì • ìƒìœ„ í•­ëª©ì— ëŒ€í•œ ì„¸ë¶€ í•­ëª©ì„ RAGë¡œ ê²€ìƒ‰í•˜ê³  í‰í‰í•œ êµ¬ì¡°ì˜ dictë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
        "    top_code = top_item.get('item_code')\n",
        "    top_name = top_item.get('item_name')\n",
        "\n",
        "    if not top_code or not top_name:\n",
        "        print(f\"â–¶ ê²½ê³ : ìƒìœ„ í•­ëª© ì •ë³´ê°€ ë¶ˆì™„ì „í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤. ë°ì´í„°: {top_item}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"â–¶ ìƒìœ„ í•­ëª© '{top_code} {top_name}'ì— ëŒ€í•œ ì„¸ë¶€ ë¶„ì„ ì‹œì‘...\")\n",
        "    sub_items_docs = find_sub_items(top_code, gri_detail_data)\n",
        "    if not sub_items_docs:\n",
        "        print(f\" -> '{top_code} {top_name}'ì— ëŒ€í•œ ì„¸ë¶€ í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        result_dict = {**top_item}\n",
        "        result_dict['relevant_sub_items'] = []\n",
        "        return result_dict\n",
        "\n",
        "    vector_store = FAISS.from_documents(sub_items_docs, embeddings)\n",
        "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
        "    relevant_docs = retriever.invoke(biz_context)\n",
        "    print(f\" -> '{top_code} {top_name}' ë¶„ì„ ì™„ë£Œ.\")\n",
        "\n",
        "    result_dict = {**top_item}\n",
        "    result_dict['relevant_sub_items'] = [doc.page_content for doc in relevant_docs]\n",
        "    return result_dict\n",
        "\n",
        "# =================================================================\n",
        "# 4) ë©”ì¸ ë¶„ì„ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜\n",
        "# =================================================================\n",
        "def match_gri(report_raw: str, model_name: str, top_n: int) -> list:\n",
        "    \"\"\"\n",
        "    ê¸°ì—…ì˜ ì›ë³¸ ë°ì´í„°(dict)ë¥¼ ì…ë ¥ë°›ì•„ GRI ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ ,\n",
        "    ìµœì¢… ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜.\n",
        "    \"\"\"\n",
        "    # â˜…â˜…â˜…â˜…â˜… ì—¬ê¸°ê°€ í•µì‹¬ ìˆ˜ì • ì‚¬í•­ (3) â˜…â˜…â˜…â˜…â˜…\n",
        "    # --- 0. ë¶„ì„ì— í•„ìš”í•œ ë°ì´í„° ë¡œë“œ ---\n",
        "    # gri_top_data = load_json(GRI_TOP_PATH) # -> ì´ì œ ì „ì—­ ë³€ìˆ˜ GRI_TOP_DEFINITIONS ì‚¬ìš©\n",
        "    gri_detail_data = load_json(GRI_DETAIL_PATH)\n",
        "    if gri_detail_data is None:\n",
        "        raise FileNotFoundError(\"GRI ì„¸ë¶€ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    biz_context = normalize(json.dumps(report_raw, ensure_ascii=False))\n",
        "\n",
        "    # --- 1. ìƒìœ„ GRI í•­ëª© íŒë³„ ---\n",
        "    top_level_result = run_stage1_find_top_items(biz_context, GRI_TOP_DEFINITIONS, MODEL_NAME, TOPN_SELECT)\n",
        "\n",
        "    # print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # --- 2. ì„¸ë¶€ í•­ëª© ë³‘ë ¬ ê²€ìƒ‰ ---\n",
        "    print('='*100)\n",
        "    print(\"Step5-4: í•˜ìœ„ GRI í•­ëª© íŒë³„\")\n",
        "    print(\"--- 2ë‹¨ê³„: RAGë¥¼ í†µí•´ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì„¸ë¶€ í•­ëª© ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘ ---\")\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    analysis_results = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        future_to_item = {\n",
        "            executor.submit(run_stage2_find_sub_item, item, gri_detail_data, biz_context, embeddings): item\n",
        "            for item in top_level_result.get('top_items', [])\n",
        "        }\n",
        "        for future in as_completed(future_to_item):\n",
        "            try:\n",
        "                result = future.result()\n",
        "                if result:\n",
        "                    analysis_results.append(result)\n",
        "            except Exception as exc:\n",
        "                item_info = future_to_item[future]\n",
        "                print(f\"'{item_info.get('item_name')}' í•­ëª© ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {exc}\")\n",
        "\n",
        "    return sorted(analysis_results, key=lambda x: x.get('item_code', ''))\n",
        "\n",
        "# =================================================================\n",
        "# ì„¹ì…˜ C: ì„±ê³¼ ì‹œê°í™” ê´€ë ¨ í•¨ìˆ˜ë“¤ (ì‹ ê·œ ì¶”ê°€)\n",
        "# =================================================================\n",
        "\n",
        "\n",
        "# ê¸°ì¡´ GraphData ëª¨ë¸ì— time_type í•„ë“œ ì¶”ê°€\n",
        "class GraphData(BaseModel):\n",
        "    title: str = Field(description=\"ê·¸ë˜í”„ì˜ ì œëª© (ì˜ˆ: 'ìˆ˜í˜œ ì¸ì› ë³€í™”')\")\n",
        "    x_axis_label: str = Field(description=\"Xì¶•ì˜ ì´ë¦„ (ì˜ˆ: 'ì—°ë„', 'ë‹¨ê³„', 'ëª©í‘œ')\")\n",
        "    y_axis_label: str = Field(description=\"Yì¶•ì˜ ì´ë¦„ (ì˜ˆ: 'ìˆ˜í˜œ ì¸ì› (ëª…)')\")\n",
        "    x_values: List[str] = Field(description=\"Xì¶•ì— í•´ë‹¹í•˜ëŠ” ê°’ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\")\n",
        "    y_values: List[float] = Field(description=\"Yì¶•ì— í•´ë‹¹í•˜ëŠ” ìˆ«ì ê°’ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\")\n",
        "    explanation: str = Field(description=\"ì¶”ì¶œëœ ì‹œê³„ì—´ ë°ì´í„°ê°€ ì˜ë¯¸í•˜ëŠ” ë°”ì— ëŒ€í•œ ê°„ê²°í•œ í•œ ì¤„ ìš”ì•½ ì„¤ëª…\")\n",
        "    time_type: str = Field(description=\"ì‹œê°„ì¶• ìœ í˜•: 'absolute' (ì ˆëŒ€ì‹œì ), 'relative' (ìƒëŒ€ë³€í™”), 'progressive' (ì ì§„ì  ëª©í‘œ)\")\n",
        "\n",
        "class GraphInfo(BaseModel):\n",
        "    graphs: List[GraphData] = Field(description=\"í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ê·¸ë˜í”„í™” ê°€ëŠ¥í•œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸. ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ []ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "# ë‹¨ì¼ ì‹œì  (KPI ìš”ì•½ìš©) ë°ì´í„° êµ¬ì¡°\n",
        "class KeyMetric(BaseModel):\n",
        "    metric_name: str = Field(description=\"í•µì‹¬ ì§€í‘œì˜ ì´ë¦„ (ì˜ˆ: 'ì˜¨ì‹¤ê°€ìŠ¤ ê°ì¶•ëŸ‰')\")\n",
        "    value: float = Field(description=\"ì§€í‘œì˜ ìˆ«ì ê°’ (ì˜ˆ: 6500)\")\n",
        "    unit: str = Field(description=\"ì§€í‘œì˜ ë‹¨ìœ„ (ì˜ˆ: 'tCO2e', 'ëª…', 'GWh')\")\n",
        "    context: str = Field(description=\"ì›ë³¸ í…ìŠ¤íŠ¸ì— ì–¸ê¸‰ëœ ì „ì²´ ë‚´ìš© (ì˜ˆ: 'ì—°ê°„ 6,500tCO2e ê°ì¶•')\")\n",
        "    explanation: str = Field(description=\"ì¶”ì¶œëœ KPIê°€ ì˜ë¯¸í•˜ëŠ” ë°”ì— ëŒ€í•œ ê°„ê²°í•œ í•œ ì¤„ ìš”ì•½ ì„¤ëª…\")\n",
        "\n",
        "class KeyMetricList(BaseModel):\n",
        "    metrics: List[KeyMetric] = Field(description=\"í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ í•µì‹¬ ì„±ê³¼ ì§€í‘œ(KPI) ë¦¬ìŠ¤íŠ¸\")\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# ## ê¸°ëŠ¥ í•¨ìˆ˜ ì •ì˜\n",
        "# =================================================================\n",
        "# --- âš ï¸ ìˆ˜ì •ëœ ë¶€ë¶„ 1: í°íŠ¸ ì°¾ëŠ” í•¨ìˆ˜ ê°œì„  ---\n",
        "def find_korean_font():\n",
        "    \"\"\"ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì•„ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (ê°œì„ ëœ ë²„ì „)\"\"\"\n",
        "    # 1. Windows: 'ë§‘ì€ ê³ ë”•'\n",
        "    if os.name == 'nt':\n",
        "        font_path = 'c:/Windows/Fonts/malgun.ttf'\n",
        "        if os.path.exists(font_path):\n",
        "            # print(f\" ìœˆë„ìš° 'ë§‘ì€ ê³ ë”•' í°íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤: {font_path}\")\n",
        "            return font_path\n",
        "\n",
        "    # 2. macOS: 'AppleGothic'\n",
        "    elif os.name == 'darwin':\n",
        "        font_path = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'\n",
        "        if os.path.exists(font_path):\n",
        "            # print(f\" macOS 'AppleGothic' í°íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤: {font_path}\")\n",
        "            return font_path\n",
        "\n",
        "    # 3. Linux/ê¸°íƒ€ í™˜ê²½: 'ë‚˜ëˆ”ê³ ë”•' ë˜ëŠ” 'ë§‘ì€ ê³ ë”•' ê³„ì—´ í°íŠ¸ ê²€ìƒ‰\n",
        "    print(\"-> ì‹œìŠ¤í…œì—ì„œ 'Nanum' ë˜ëŠ” 'Malgun' ê³„ì—´ í°íŠ¸ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...\")\n",
        "    for font in fm.findSystemFonts(fontpaths=None, fontext='ttf'):\n",
        "        try:\n",
        "            # fm.FontPropertiesë¥¼ ì‚¬ìš©í•˜ì—¬ í°íŠ¸ì˜ ì‹¤ì œ ì´ë¦„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "            font_name = fm.FontProperties(fname=font).get_name()\n",
        "            if 'nanum' in font_name.lower() or 'malgun' in font_name.lower():\n",
        "                # print(f\" ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤: {font}\")\n",
        "                return font\n",
        "        except RuntimeError:\n",
        "            # ì¼ë¶€ ì˜ëª»ëœ í°íŠ¸ íŒŒì¼ì€ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆì–´ ì˜ˆì™¸ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "            continue\n",
        "\n",
        "    print(\"âŒ ê²½ê³ : ì‹œìŠ¤í…œì—ì„œ 'ë§‘ì€ ê³ ë”•'ì´ë‚˜ 'ë‚˜ëˆ”ê³ ë”•'ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê·¸ë˜í”„ì˜ í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "    return None\n",
        "# -----------------------------------------------------------------\n",
        "# ### ì‹œê³„ì—´ ë³€í™” (ê·¸ë˜í”„) ê´€ë ¨ í•¨ìˆ˜ë“¤\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "\n",
        "def extract_graph_data(final_json_result: dict) -> Optional[List[dict]]:\n",
        "    \"\"\"[ê°œì„ ëœ ì‹œê³„ì—´] ì ˆëŒ€ì‹œì ê³¼ ìƒëŒ€ë³€í™”ë¥¼ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
        "    print(\"--- ê°œì„ ëœ ì‹œê³„ì—´ ë°ì´í„° ì¶”ì¶œ ì‹œì‘ ---\")\n",
        "    text_to_analyze = \"\\n\\n\".join([f\"## {k}\\n{v}\" for k, v in final_json_result.items() if isinstance(v, str)])\n",
        "\n",
        "    parser = JsonOutputParser(pydantic_object=GraphInfo)\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        template=\"\"\"ë‹¹ì‹ ì€ ì •ë°€í•œ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ ì •ëŸ‰ì  ë³€í™” ë°ì´í„°ë¥¼ ì°¾ë˜, **ì‹œê°„ì¶•ì˜ ì„±ê²©ì„ ì •í™•íˆ êµ¬ë¶„**í•˜ì—¬ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "## ğŸ” ì‹œê°„ì¶• ë¶„ë¥˜ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”)\n",
        "\n",
        "**1. ABSOLUTE (ì ˆëŒ€ì‹œì ) - time_type: \"absolute\"**\n",
        "- ì¡°ê±´: ê° ìˆ˜ì¹˜ì— ëª…í™•í•œ ì—°ë„/ë¶„ê¸°/ì›”ì´ í‘œê¸°ë¨\n",
        "- ì˜ˆì‹œ: \"2023ë…„ 460ëª… â†’ 2024ë…„ 780ëª… â†’ 2025ë…„ 1,200ëª…\"\n",
        "- Xì¶•: ì‹¤ì œ ì—°ë„/ë¶„ê¸°/ì›” ì‚¬ìš©\n",
        "- ì²˜ë¦¬: ê·¸ëŒ€ë¡œ ì¶”ì¶œ\n",
        "\n",
        "**2. RELATIVE (ìƒëŒ€ë³€í™”) - time_type: \"relative\"**\n",
        "- ì¡°ê±´: ì‹œì‘â†’ëª©í‘œ í˜•íƒœì´ì§€ë§Œ êµ¬ì²´ì  ì‹œì ì´ ì—†ìŒ\n",
        "- ì˜ˆì‹œ: \"62% â†’ 75%\", \"48% â†’ 62% â†’ 70%\"\n",
        "- Xì¶•: \"í˜„ì¬\", \"1ë‹¨ê³„ ëª©í‘œ\", \"2ë‹¨ê³„ ëª©í‘œ\", \"3ë‹¨ê³„ ëª©í‘œ\" ë“±ìœ¼ë¡œ í‘œí˜„\n",
        "- âš ï¸ **ì ˆëŒ€ ì„ì˜ì˜ ì—°ë„ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”!**\n",
        "\n",
        "**3. PROGRESSIVE (ì ì§„ì  ëª©í‘œ) - time_type: \"progressive\"**\n",
        "- ì¡°ê±´: 3ê°œ ì´ìƒì˜ ë‹¨ê³„ì  ëª©í‘œ ìˆ˜ì¹˜ (íŠ¹íˆ ê¸‰ì—¬, ë§¤ì¶œ ë“±)\n",
        "- ì˜ˆì‹œ: \"ì´ˆì„ í‰ê·  ì›” 2.3M â†’ 2.6M â†’ 2.8M\"\n",
        "- Xì¶•: ì ì§„ì  í–¥ìƒì„ ë‚˜íƒ€ë‚´ëŠ” ë¼ë²¨\n",
        "- ë¼ë²¨ ì˜ˆì‹œ: [\"1ì°¨ë…„ë„\", \"2ì°¨ë…„ë„\", \"3ì°¨ë…„ë„\"] or [í˜„ì¬, 1ì°¨ ëª©í‘œ, 2ì°¨ ëª©í‘œ,..., ìµœì¢…ëª©í‘œ]\n",
        "\n",
        "## ğŸ“‹ ì¶”ì¶œ ì§€ì¹¨\n",
        "1. **ìµœì†Œ ë°ì´í„° ê°œìˆ˜ ê·œì¹™ (ê°€ì¥ ì¤‘ìš”)**: `y_values` ë¦¬ìŠ¤íŠ¸ì—ëŠ” ë°˜ë“œì‹œ **2ê°œ ì´ìƒì˜ ìˆ«ì ê°’**ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë°ì´í„° í¬ì¸íŠ¸ê°€ í•˜ë‚˜ë¿ì¸ ê²½ìš°ëŠ” **ì ˆëŒ€** ì¶”ì¶œ ëŒ€ìƒì´ ì•„ë‹™ë‹ˆë‹¤.\n",
        "\n",
        "2. **ë°ì´í„° ë¶„ë¥˜ ê³¼ì •**:\n",
        "   - ê° ë°ì´í„°ë¥¼ ìœ„ 3ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜\n",
        "   - time_type í•„ë“œì— ì •í™•í•œ ë¶„ë¥˜ ê¸°ë¡\n",
        "\n",
        "3. **Xì¶• ë¼ë²¨ë§ ê·œì¹™**:\n",
        "   - ABSOLUTE: ì›ë¬¸ì˜ ì—°ë„/ë¶„ê¸°/ì›” ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
        "   - RELATIVE: \"í˜„ì¬\" â†’ \"ëª©í‘œ\" ë˜ëŠ” \"1ë‹¨ê³„\" â†’ \"2ë‹¨ê³„\" â†’ \"ìµœì¢…ë‹¨ê³„\" ë˜ëŠ” \"1ì°¨ë…„ë„\" â†’  \"2ì°¨ë…„ë„\" â†’  \"3ì°¨ë…„ë„\"\n",
        "   - PROGRESSIVE: ë§¥ë½ì— ë§ëŠ” ì ì§„ì  í‘œí˜„ ì‚¬ìš©\n",
        "\n",
        "4. **ì¶”ì¶œ ëŒ€ìƒ**:\n",
        "   - 2ê°œ ì´ìƒì˜ ìˆ˜ì¹˜ê°€ ì—°ê²°ëœ ë³€í™” ë°ì´í„°\n",
        "   - **ì‹œê°„ì˜ 'ë³€í™”'ë§Œ ì¶”ì¶œ**: ë°˜ë“œì‹œ ë™ì¼í•œ ì§€í‘œê°€ ì‹œê°„(ì—°ë„, ë¶„ê¸°, ë‹¨ê³„ ë“±)ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” ë°ì´í„°ë§Œ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "   - ê° ìˆ˜ì¹˜ëŠ” ë™ì¼í•œ ë‹¨ìœ„ì—¬ì•¼ í•¨\n",
        "\n",
        "5. **ì¶”ì¶œ ê¸ˆì§€**:\n",
        "   - ë‹¨ì¼ ìˆ˜ì¹˜ (\"ì—°ê°„ 12,000 kWh\")\n",
        "   - ì‹œê°„ íë¦„ì´ ì—†ëŠ” ë¹„êµ ë°ì´í„°\n",
        "\n",
        "6. **`explanation` ì‘ì„± ê·œì¹™ (ë§¤ìš° ì¤‘ìš”)**:\n",
        "   - ì›ë³¸ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, **ì‹œì‘ ê°’ê³¼ ìµœì¢… ê°’ì„ ë°˜ë“œì‹œ í¬í•¨**í•˜ëŠ” í•œ ë¬¸ì¥ì˜ ì„¤ëª…ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
        "   - ì˜ˆì‹œ: \"ì´ˆì„ í‰ê· ì´ ì›” 2,300,000ì›ì—ì„œ ìµœì¢… ëª©í‘œ 2,800,000ì›ìœ¼ë¡œ ìƒìŠ¹í•˜ëŠ” ê¸°ëŒ€/ëª©í‘œ/ì˜ˆì •.\"\n",
        "\n",
        "\n",
        "## ğŸ’¡ ì˜ˆì‹œ\n",
        "\n",
        "**ì…ë ¥**: \"6ê°œì›” ì·¨ì—…ë¥  ëª©í‘œ 62% â†’ 75%\"\n",
        "**ì˜¬ë°”ë¥¸ ì¶œë ¥**:\n",
        "```json\n",
        "{{\n",
        "  \"title\": \"6ê°œì›” ì·¨ì—…ë¥  ëª©í‘œ ë³€í™”\",\n",
        "  \"x_axis_label\": \"ëª©í‘œ ë‹¨ê³„\",\n",
        "  \"x_values\": [\"í˜„ì¬ ëª©í‘œ\", \"ê°œì„  ëª©í‘œ\"],\n",
        "  \"y_values\": [62, 75],\n",
        "  \"time_type\": \"relative\"\n",
        "}}\n",
        "```\n",
        "\n",
        "**ì…ë ¥**: \"ì°¸ì—¬ì ìˆ˜: 2023ë…„ 460ëª… â†’ 2024ë…„ 780ëª… â†’ 2025ë…„ 1,200ëª…\"\n",
        "**ì˜¬ë°”ë¥¸ ì¶œë ¥**:\n",
        "```json\n",
        "{{\n",
        "  \"title\": \"ì—°ë„ë³„ ì°¸ì—¬ì ìˆ˜ ë³€í™”\",\n",
        "  \"x_axis_label\": \"ì—°ë„\",\n",
        "  \"x_values\": [\"2023\", \"2024\", \"2025\"],\n",
        "  \"y_values\": [460, 780, 1200],\n",
        "  \"time_type\": \"absolute\"\n",
        "}}\n",
        "```\n",
        "\n",
        "[ë¶„ì„í•  í…ìŠ¤íŠ¸]\n",
        "{text}\n",
        "\n",
        "[ì¶œë ¥ í˜•ì‹]\n",
        "{format_instructions}\"\"\",\n",
        "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        "    )\n",
        "\n",
        "    chain = prompt | llm | parser\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": text_to_analyze})\n",
        "        graphs_list = result.get('graphs', [])\n",
        "\n",
        "        if graphs_list:\n",
        "            print(f\"--- ì´ {len(graphs_list)}ê°œì˜ ê·¸ë˜í”„ ë°ì´í„°ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤ ---\")\n",
        "            # ë¶„ë¥˜ë³„ ê°œìˆ˜ ì¶œë ¥\n",
        "            absolute_count = sum(1 for g in graphs_list if g.get('time_type') == 'absolute')\n",
        "            relative_count = sum(1 for g in graphs_list if g.get('time_type') == 'relative')\n",
        "            progressive_count = sum(1 for g in graphs_list if g.get('time_type') == 'progressive')\n",
        "\n",
        "            print(f\"  - ì ˆëŒ€ì‹œì (absolute): {absolute_count}ê°œ\")\n",
        "            print(f\"  - ìƒëŒ€ë³€í™”(relative): {relative_count}ê°œ\")\n",
        "            print(f\"  - ì ì§„ì  ëª©í‘œ(progressive): {progressive_count}ê°œ\")\n",
        "\n",
        "            return graphs_list\n",
        "        else:\n",
        "            print(\"--- ê·¸ë˜í”„ë¡œ í‘œí˜„í•  ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ ---\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"--- ì˜¤ë¥˜: ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e} ---\")\n",
        "        return None\n",
        "\n",
        "def extract_key_metrics(final_json_result: dict) -> Optional[List[dict]]:\n",
        "    \"\"\"[ë‹¨ì¼ ì‹œì ] LLMì„ ì‚¬ìš©í•´ KPI ë°ì´í„°ì™€ 'í•œ ì¤„ ì„¤ëª…'ì„ ëª…í™•í•œ ê·œì¹™ì— ë”°ë¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
        "    print(\"--- 8-1ë‹¨ê³„: í•µì‹¬ ì„±ê³¼ ì§€í‘œ(KPI) ì¶”ì¶œ ì‹œì‘ ---\")\n",
        "    text_to_analyze = \"\\n\\n\".join([ f\"## {k}\\n{v}\" for k, v in final_json_result.items() if isinstance(v, str) ])\n",
        "    parser = JsonOutputParser(pydantic_object=KeyMetricList)\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        template=\"\"\"ë‹¹ì‹ ì€ ESG ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬, íšŒì‚¬ì˜ í•µì‹¬ì ì¸ ì„±ê³¼ë¥¼ ë‚˜íƒ€ë‚´ëŠ” **'ì‹œê°„ ë³€í™”ê°€ ì—†ëŠ”' ë‹¨ì¼ ì •ëŸ‰ ì§€í‘œ(KPI)**ë¥¼ ëª¨ë‘ ì°¾ì•„ JSON ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì§€ì¹¨:\n",
        "1. 'ì—°ê°„', 'ì´', 'ì•½' ë“±ì´ ë¶™ì€ ë‹¨ì¼ ìˆ«ì ì„±ê³¼ë¥¼ ëª¨ë‘ ì°¾ìœ¼ì„¸ìš”.\n",
        "2. **ì‹œê°„ì— ë”°ë¥¸ ë³€í™”ê°€ ìˆëŠ” ë°ì´í„°ëŠ” ë°˜ë“œì‹œ ì œì™¸**í•˜ì„¸ìš”.\n",
        "3. ê° ì§€í‘œì— ëŒ€í•´ 'metric_name', 'value', 'unit', 'context'ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”.\n",
        "4. ê°’(value)ì€ ìˆ«ìë§Œ ë‚¨ê¸°ê³ , ë‹¨ìœ„(unit)ëŠ” í…ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.\n",
        "5. **explanation ì‘ì„± ê·œì¹™ (ê°€ì¥ ì¤‘ìš”)**\n",
        "   ì›ë¬¸ ë‰˜ì•™ìŠ¤ë¥¼ ë³´ê³  ë°˜ë“œì‹œ ë‹¤ìŒ ì„¸ ê°€ì§€ í˜•ì‹ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì—¬ ì„¤ëª…ì„ ì‘ì„±í•©ë‹ˆë‹¤:\n",
        "   - **ëª©í‘œì¹˜(Target)**: ì›ë¬¸ì— \"ëª©í‘œ\", \"~í•  ê²ƒ\", \"ê¸°ëŒ€\", \"ì „ë§\", \"ê³„íš\" ë“±ì˜ í‘œí˜„ì´ ìˆì„ ê²½ìš° -> **\"ëª©í‘œì¹˜ â€” â€¦ ë‹¬ì„±ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.\"** í˜•ì‹ìœ¼ë¡œ ì‘ì„±\n",
        "   - **ì„±ê³¼ì¹˜(Result)**: ì›ë¬¸ì— \"ë‹¬ì„±í–ˆë‹¤\", \"ê°ì†Œí–ˆë‹¤\", \"ì ˆê°í–ˆë‹¤\" ë“± ëª…í™•í•œ ê³¼ê±°í˜• í‘œí˜„ì´ ìˆì„ ê²½ìš° -> **\"ì„±ê³¼ì¹˜ â€” â€¦ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.\"** í˜•ì‹ìœ¼ë¡œ ì‘ì„±\n",
        "   - **í˜„í™©ì¹˜(Status)**: í˜„ì¬ ìƒí™©ì´ë‚˜ ìŠ¤í™ì„ ì„¤ëª…í•˜ê±°ë‚˜ ìœ„ ë‘ ì¡°ê±´ì— í•´ë‹¹í•˜ì§€ ì•Šì„ ê²½ìš° -> **\"í˜„í™©ì¹˜ â€” â€¦ë¡œ ì œì‹œëœ ìˆ˜ì¹˜ì…ë‹ˆë‹¤.\"** í˜•ì‹ìœ¼ë¡œ ì‘ì„±\n",
        "6. ì¶”ì¶œí•  ë°ì´í„°ê°€ ì „í˜€ ì—†ìœ¼ë©´, 'metrics' í•„ë“œë¥¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸ `[]`ë¡œ ì„¤ì •í•˜ì—¬ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "[ë¶„ì„í•  í…ìŠ¤íŠ¸]\n",
        "{text}\n",
        "[ì¶œë ¥ í˜•ì‹]\n",
        "{format_instructions}\"\"\",\n",
        "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        "    )\n",
        "    chain = prompt | llm | parser\n",
        "    try:\n",
        "        result = chain.invoke({\"text\": text_to_analyze})\n",
        "        metrics_list = result.get('metrics', [])\n",
        "        if metrics_list:\n",
        "            print(f\"--- 8-1ë‹¨ê³„: ì´ {len(metrics_list)}ê°œì˜ í•µì‹¬ ì§€í‘œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ---\")\n",
        "            return metrics_list\n",
        "        else:\n",
        "            print(\"--- 8-1ë‹¨ê³„: ì¶”ì¶œí•  í•µì‹¬ ì§€í‘œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ---\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"--- 8-1ë‹¨ê³„ ì˜¤ë¥˜: KPI ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e} ---\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def create_and_save_graphs_v3(graph_data: Optional[List[dict]], save_dir: str = 'result/graphs'):\n",
        "    \"\"\"[ìµœì¢… ì‹œê°í™”] v2ì—ì„œ Yê°’ í…ìŠ¤íŠ¸ë¥¼ ê°•ì¡°í•˜ì—¬ ê°€ë…ì„±ì„ ê·¹ëŒ€í™”í•œ ë²„ì „\"\"\"\n",
        "    if not graph_data:\n",
        "        print(\"--- ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ì–´ ê·¸ë˜í”„ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤ ---\")\n",
        "        return\n",
        "    print('='*100)\n",
        "    print(\"Step 7-2 ìµœì¢… ë””ìì¸ìœ¼ë¡œ ë§ì¶¤ ê·¸ë˜í”„ ìƒì„± ì‹œì‘  ---\")\n",
        "\n",
        "    font_path = find_korean_font()\n",
        "    font_prop = None\n",
        "    if font_path:\n",
        "        font_prop = fm.FontProperties(fname=font_path)\n",
        "        # print(f\" í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ: {font_prop.get_name()}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for i, data in enumerate(graph_data):\n",
        "        try:\n",
        "            fig, ax = plt.subplots(figsize=(12, 8))\n",
        "            fig.patch.set_facecolor('#F8F9FA')\n",
        "            ax.set_facecolor('#F8F9FA')\n",
        "\n",
        "            time_type = data.get('time_type', 'relative')\n",
        "\n",
        "            colors = ['#ff8a80', '#80bfff', '#8aff80', '#ffd700', '#b19cd9']\n",
        "            bar_colors = [colors[j % len(colors)] for j in range(len(data['x_values']))]\n",
        "\n",
        "            bars = ax.bar(data['x_values'], data['y_values'], color=bar_colors, width=0.6, alpha=0.9)\n",
        "\n",
        "            max_y_value = max(data['y_values']) if data['y_values'] else 1\n",
        "\n",
        "            for bar, value, x_val, bar_color in zip(bars, data['y_values'], data['x_values'], bar_colors):\n",
        "\n",
        "                # Yê°’(ìˆ«ì)ì€ ê¸°ì¡´ì²˜ëŸ¼ ë§‰ëŒ€ ë°”ê¹¥ ìƒë‹¨ì— í‘œì‹œ (ìœ ì§€)\n",
        "                formatted_value = f'{value:,.0f}'\n",
        "                unit_match = re.search(r'\\((.*?)\\)', data['y_axis_label'])\n",
        "                unit = unit_match.group(1).strip() if unit_match else ''\n",
        "                label_text = f'{formatted_value}{unit}'\n",
        "\n",
        "                ax.text(bar.get_x() + bar.get_width()/2,\n",
        "                        bar.get_height() + max_y_value * 0.02,\n",
        "                        label_text, ha='center', va='bottom', fontsize=18, fontweight='bold',\n",
        "                        color=bar_color, fontproperties=font_prop)\n",
        "\n",
        "            # ì œëª©, ì„¤ëª… ë“±ì€ ê¸°ì¡´ê³¼ ë™ì¼\n",
        "            type_label = {'absolute': '[ì—°ë„ë³„]', 'relative': '[ëª©í‘œ ì§„í–‰]', 'stage': '[ë‹¨ê³„ë³„ ëª©í‘œ]'}\n",
        "            full_title = f\"{type_label.get(time_type, '')} {data['title']}\"\n",
        "            ax.text(0.5, 1.08, full_title, transform=ax.transAxes, ha='center', va='top', fontsize=24, fontweight='bold', fontproperties=font_prop, color='#1A237E')\n",
        "            props = dict(boxstyle='round,pad=0.5', facecolor='#E0F7FA', alpha=0.8, ec='none')\n",
        "            ax.text(0.5, 1.0, data['explanation'], transform=ax.transAxes, ha='center', va='top', fontsize=13, fontproperties=font_prop, bbox=props, color='#006064')\n",
        "\n",
        "            ax.set_ylim(0, max_y_value * 1.3)\n",
        "            ax.set_xticks(range(len(data['x_values'])))\n",
        "\n",
        "            # ##################################################################\n",
        "            # ### â–¼â–¼â–¼ Xì¶• ë¼ë²¨ í¬ê¸° ë° êµµê¸° ìˆ˜ì • â–¼â–¼â–¼                       ###\n",
        "            # ##################################################################\n",
        "            ax.set_xticklabels(\n",
        "                data['x_values'],\n",
        "                fontproperties=font_prop,\n",
        "                fontsize=20,        # 15 -> 20 ìœ¼ë¡œ í¬ê¸° ì¦ê°€\n",
        "                fontweight='heavy',   # 'bold' -> 'heavy' ë¡œ ë” ë‘ê»ê²Œ\n",
        "                color='#333'\n",
        "            )\n",
        "            # ##################################################################\n",
        "\n",
        "            ax.set_yticks([])\n",
        "            ax.spines[['top', 'right', 'left']].set_visible(False)\n",
        "            ax.spines['bottom'].set_visible(True)\n",
        "            ax.spines['bottom'].set_color('#CDCDCD')\n",
        "            ax.tick_params(axis='x', length=0, pad=10)\n",
        "            ax.grid(False)\n",
        "\n",
        "            plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
        "\n",
        "            safe_filename = re.sub(r'[\\\\/*?:\"<>|]', \"\", data['title'])\n",
        "            save_path = os.path.join(save_dir, f\"{time_type}_{i+1}_{safe_filename}.png\")\n",
        "            plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            print(f\"[{time_type.upper()}] ê·¸ë˜í”„ ì €ì¥: '{save_path}'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ '{data['title']}' ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "            plt.close()\n",
        "\n",
        "# print(\" ëª¨ë“  í•¨ìˆ˜ ë° í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ.\")\n",
        "\n",
        "def process_graph_data(final_json_data: dict):\n",
        "    \"\"\"[ë³‘ë ¬ ì‘ì—… 1] Step 7: ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
        "    print('='*100)\n",
        "    print(\" Step 7-1 (ê·¸ë˜í”„ ì²˜ë¦¬) ì‹œì‘...\")\n",
        "\n",
        "    # 1. ë¨¼ì € ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "    graphable_data = extract_graph_data(final_json_data)\n",
        "\n",
        "    # 2. ë°ì´í„°ê°€ ìˆëŠ”ì§€ ì—†ëŠ”ì§€ë¶€í„° í™•ì¸í•©ë‹ˆë‹¤.\n",
        "    if not graphable_data:\n",
        "        print(\" Step 7: ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ì–´ ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\")\n",
        "        return None # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì—¬ê¸°ì„œ ì¦‰ì‹œ í•¨ìˆ˜ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\n",
        "\n",
        "    # 3. ë°ì´í„°ê°€ ìˆëŠ” ê²ƒì´ í™•ì¸ëœ í›„ì—ë§Œ ì•„ë˜ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "    # print(\" Step 7: ê·¸ë˜í”„ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ. íŒŒì¼ ì €ì¥ ë° ì‹œê°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "    # íŒŒì¼ ì €ì¥\n",
        "    with open('result/graph_data.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(graphable_data, f, ensure_ascii=False, indent=4)\n",
        "    print(\" 'graph_data.json' ì €ì¥ ì™„ë£Œ.\")\n",
        "\n",
        "    # ëˆ„ë½ë˜ì—ˆë˜ ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
        "    create_and_save_graphs_v3(graphable_data, save_dir='result/graphs')\n",
        "\n",
        "    print(\"(ê·¸ë˜í”„ ì²˜ë¦¬) ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ.\")\n",
        "    return \"Graph processing complete\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "    # =================================================================\n",
        "    # ### 1ë‹¨ê³„: ë¬¸ì„œ ë¶„ì„ ë° JSON ë³´ê³ ì„œ ìƒì„±\n",
        "    # =================================================================\n",
        "    company_info = read_document(DOCUMENT_PATH)\n",
        "    masked_company_info = mask_info(company_info)\n",
        "    company_summary = summary_sv(masked_company_info)\n",
        "    sdgs_main = match_sdgs_by_llm(company_summary)\n",
        "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        future_sdgs_sub = executor.submit(match_sdgs_sub_by_llm, sdgs_main)\n",
        "        future_raw_report = executor.submit(generate_raw_report, masked_company_info, sdgs_main)\n",
        "\n",
        "        sdgs_sub = future_sdgs_sub.result()\n",
        "        raw_report = future_raw_report.result()\n",
        "\n",
        "    gri_main = match_gri(raw_report, MODEL_NAME, TOPN_SELECT)\n",
        "    # ## ê²€ì¦\n",
        "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        future_agent = executor.submit(process_raw_report_agent, raw_report)\n",
        "        future_summary = executor.submit(summarize_validated_report, raw_report, sdgs_main)\n",
        "\n",
        "        ai_explain, validated_report_list = future_agent.result()\n",
        "        ai_summary = future_summary.result()\n",
        "\n",
        "    final_json = to_json(company_summary, sdgs_main, sdgs_sub, gri_main,ai_explain, validated_report_list, ai_summary)\n",
        "    with open('result/result.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(final_json, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(\"--- 1ë‹¨ê³„ ì™„ë£Œ: ìµœì¢… JSON ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤ ---\\n\")\n",
        "\n",
        "    # =================================================================\n",
        "    # ### 2ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ ë° ì‹œê°í™”\n",
        "    # =================================================================\n",
        "    print(\"--- 2ë‹¨ê³„: ë°ì´í„° ì¶”ì¶œ ë° ì‹œê°í™” ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ---\")\n",
        "\n",
        "    # 1ë‹¨ê³„ì—ì„œ ìƒì„±ëœ final_jsonì„ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰\n",
        "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        # Step 7 (ê·¸ë˜í”„ ì²˜ë¦¬)ì™€ Step 8 (KPI ì²˜ë¦¬)ì„ ë™ì‹œì— ì‹¤í–‰\n",
        "        future_graphs = executor.submit(process_graph_data, final_json)\n",
        "        # future_kpis = executor.submit(process_kpi_data, final_json)\n",
        "\n",
        "        # ë‘ ì‘ì—…ì´ ëª¨ë‘ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°\n",
        "        graph_result = future_graphs.result()\n",
        "        # kpi_result = future_kpis.result()\n",
        "\n",
        "    print(\"\\n--- ëª¨ë“  ë¶„ì„ ë° ì‹œê°í™” ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ ---\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}